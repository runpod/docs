---
title: "Concepts"
description: "Key concepts and terminology for understanding Runpod's platform and products."
---

## [Runpod console](https://console.runpod.io)

The web interface for managing your compute resources, account, teams, and billing.

## [Serverless](/serverless/overview)

A pay-as-you-go compute solution designed for dynamic autoscaling in production AI/ML apps.

## [Pod](/pods/overview)

A dedicated GPU or CPU instance for containerized AI/ML workloads, such as training models, running inference, or other compute-intensive tasks.

## [Public Endpoint](/hub/public-endpoints)

An AI model API hosted by Runpod that you can access directly without deploying your own infrastructure.

## [Instant Cluster](/instant-clusters)

A managed compute cluster with high-speed networking for multi-node distributed workloads like training large AI models.

## [Network volume](/storage/network-volumes)

Persistent storage that exists independently of your other compute resources and can be attached to multiple Pods or Serverless endpoints to share data between machines.

## [S3-compatible API](/storage/s3-api)

A storage interface compatible with Amazon S3 for uploading, downloading, and managing files in your network volumes.

## [Runpod Hub](/hub/overview)

A repository for discovering, deploying, and sharing preconfigured AI projects optimized for Runpod.

## Container

A Docker-based environment that packages your code, dependencies, and runtime into a portable unit that runs consistently across machines.

## Data center

Physical facilities where Runpod's GPU and CPU hardware is located. Your choice of data center can affect latency, available GPU types, and pricing.

## Machine

The physical server hardware within a data center that hosts your workloads. Each machine contains CPUs, GPUs, memory, and storage.
