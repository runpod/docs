---
title: "Workflow examples"
sidebarTitle: "Workflow examples"
description: "Examples of how you can use Runpod's compute services to build your AI/ML application."
---

## Develop-to-deploy cycle

**Goal:** Build a custom AI application from scratch and ship it to production.

1. **Interactive development:** Deploy a single [Pod](/pods/overview) with a GPU to act as your cloud workstation. Connect via [VSCode](/pods/configuration/connect-to-ide) or [JupyterLab](/pods/connect-to-a-pod#jupyterlab-connection) to write code, load models from Hugging Face, install dependencies, and debug your inference logic in real-time.
2. **Containerization:** Once your code is working, move your basic inference logic to a Serverless [handler function](/serverless/workers/handler-functions), then build a [Docker image](/serverless/workers/create-dockerfile) containing your application and dependencies and [push it to a container registry](/serverless/workers/deploy).
3. **Production deployment:** Deploy the Docker image as a [Serverless endpoint](/serverless/overview). Start [sending requests](/serverless/endpoints/send-requests) to your application; it will automatically scale up GPU workers as needed, and scale down to zero when idle.

## Distributed training for an LLM

**Goal:** Fine-tune a massive LLM (70B+) and serve it immediately without moving data.

1. **Multi-node training:** You spin up an [Instant Cluster](/instant-clusters) with 16x H100 GPUs to fine-tune a Llama-3-70B model using FSDP or DeepSpeed.
2. **Unified storage:** Throughout training, checkpoints and the final model weights are saved directly to a [network volume](/storage/network-volumes) attached to the cluster.
3. **Instant serving:** You deploy a [vLLM Serverless worker](/serverless/vllm/overview) and mount that *same* network volume. The endpoint reads the model weights directly from storage, allowing you to serve your newly trained model via API minutes after training finishes.

## Startup MVP

**Goal:** Launch a GenAI avatar app quickly with minimal DevOps overhead.

1. **Prototype with Public Endpoints:** You validate your product idea using the Flux [Public Endpoint](/hub/public-endpoints) to generate images. This requires zero infrastructure setup; you simply pay per image generated.
2. **Scale with Serverless:** As you grow, you need a unique art style. You fine-tune a model and deploy it as a [Serverless endpoint](/serverless/overview). This allows your app to handle traffic spikes automatically while scaling down to zero costs during quiet hours.

### Interactive research loop

**Goal:** Experiment with new model architectures using large datasets.

1. **Explore on a Pod:** Spin up a single-GPU [Pod](/pods/overview) with JupyterLab enabled. Mount a [network volume](/storage/network-volumes) to hold your 2TB dataset.
2. **Iterate code:** Write and debug your training loop interactively in the Pod. If the process crashes, the Pod restarts quickly, and your data remains safe on the network volume.
3. **Scale up:** Once the code is stable, you don't need to move the data. You terminate the single Pod and spin up an [Instant Cluster](/instant-clusters) attached to that *same* network volume to run the full training job across multiple nodes.

## Batch processing job

**Goal:** Process 10,000 video files for a media company.

1. **Queue requests:** Your backend pushes 10,000 job payloads to a [Serverless Endpoint](/serverless/overview) configured as an asynchronous queue.
2. **Auto-scale:** The endpoint detects the queue depth and automatically spins up 50 concurrent workers (e.g., L4 GPUs) to process the videos in parallel.
3. **Cost optimization:** As the queue drains, the workers scale down to zero automatically. You pay only for the exact GPU seconds used to process the videos, with no idle server costs.

## Enterprise fine-tuning factory

**Goal:** Regularly fine-tune models on new customer data automatically.

1. **Data ingestion:** Customer data is uploaded to a shared [network volume](/storage/network-volumes).
2. **Programmatic training:** A script uses the [Runpod API](/api-reference/pods/POST/pods) to spin up a fresh on-demand Pod.
3. **Execution:** The Pod mounts the volume, runs the training script, saves the new model weights back to the volume, and then [terminates itself](/pods/manage-pods#terminate-a-pod) via API call to stop billing immediately.
4. **Hot reload:** A separate [Serverless endpoint](/serverless/overview) is triggered to reload the new weights from the volume (or [update the cached model](/serverless/endpoints/model-caching)), making the new model available for inference immediately.