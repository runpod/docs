---
title: "Runpod product overview"
sidebarTitle: "Product overview"
description: "Explore Runpod's major offerings and find the right solution for your workload."
---

Runpod offers cloud computing resources for AI and machine learning workloads. You can choose from instant GPUs for development, auto-scaling Serverless computing, pre-deployed AI models, or multi-node clusters for distributed training.

## [Serverless](/serverless/overview)

Serverless provides pay-per-second computing with automatic scaling for production AI workloads. You only pay for actual compute time when your code runs, with no idle costs, making Serverless ideal for variable workloads and cost-efficient production deployments.

## [Pods](/pods/overview)

Pods give you dedicated GPU or CPU instances for containerized workloads. Pods are billed by the minute and stay available as long as you keep them running, making them perfect for development, training, and workloads that need continuous access.

## [Public Endpoints](/hub/public-endpoints)

Public Endpoints provide instant API access to pre-deployed AI models for image, video, and text generation without any setup. You only pay for what you generate, making it easy to integrate AI into your applications without managing infrastructure.

## [Instant Clusters](/instant-clusters)

Instant Clusters deliver fully managed multi-node compute clusters for large-scale distributed workloads. With high-speed networking between nodes, you can run multi-node training, fine-tune large language models, and handle other tasks that require multiple GPUs working in parallel.


## Choosing the right option

Choose **Serverless** when you need auto-scaling for inference workloads with variable traffic. Pay-per-second billing minimizes costs, and automatic worker management handles unpredictable workloads and API services efficiently.

Choose **Pods** when you need full control for development and experimentation. They work best for training models, iterative development, and custom workflows that require persistent storage and long-running processes.

Choose **Public Endpoints** when you want to quickly integrate AI capabilities without managing infrastructure. They're ideal for prototyping and production applications that use popular AI models with simple pay-per-use pricing.

Choose **Instant Clusters** when your workload requires multiple GPUs across multiple nodes. They provide the infrastructure for training or fine-tuning large language models and other distributed computing tasks with high-speed networking.

You can combine these products as well. For example, use Pods for development and experimentation, Serverless for production inference, and Instant Clusters for large-scale training runs.
