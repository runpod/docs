---
title: "Run Google's Gemma 3 model"
sidebarTitle: "Run Gemma 3"
description: "Deploy Google's Gemma 3 model on Runpod Serverless using the vLLM worker and build an interactive chatbot."
---

This tutorial walks you through deploying Google's Gemma 3 model on Runpod Serverless using the vLLM worker. You'll deploy the `gemma-3-1b-it` instruction-tuned variant, a lightweight model that runs efficiently on a variety of GPUs.

By the end, you'll have a fully functional Serverless endpoint that can respond to chat-style prompts through an OpenAI-compatible API.

## What you'll learn

- How to accept Google's terms for gated models on Hugging Face.
- How to deploy a vLLM worker from the Runpod Hub.
- How to configure environment variables for gated models.
- How to interact with your endpoint using the OpenAI Python client.
- How to build a simple command-line chatbot.

## Requirements

Before starting this tutorial, you'll need:

- A [Runpod account](https://www.runpod.io/console/signup) with available credits.
- A [Runpod API key](/get-started/api-keys).
- A [Hugging Face account](https://huggingface.co/join) and [access token](https://huggingface.co/settings/tokens).
- Python 3.8+ installed on your local machine.
- The OpenAI Python client (`pip install openai`).

## Step 1: Accept Google's terms on Hugging Face

Gemma 3 is a gated model on Hugging Face. Before deploying, go to the [Gemma 3 1B model page](https://huggingface.co/google/gemma-3-1b-it) and click **Agree and access repository** while logged into your Hugging Face account.

<Warning>
You must accept the terms using the same Hugging Face account that you'll use to generate your access token. Without accepting these terms, your deployment will fail to download the model.
</Warning>

## Step 2: Deploy the vLLM worker

Deploy a vLLM worker through the Runpod Hub with Gemma 3 as your model:

1. Open the [vLLM worker listing](https://console.runpod.io/hub/runpod-workers/worker-vllm) in the Runpod Hub.
2. Click **Deploy**, using the latest vLLM worker version.
3. In the **Model** field, enter: `google/gemma-3-1b-it`.
4. Click **Advanced** to expand the vLLM settings.
5. Set **Max Model Length** to `8192`.
6. Click **Next**.
7. Under **Public Environment Variables**, click **Add Variable** and add:
   - **Key**: `HF_TOKEN`
   - **Value**: Your Hugging Face access token.
8. Select a GPU with at least 16GB of VRAM (such as RTX A4000 or RTX 4000 Ada).
9. Click **Create Endpoint**.

<Note>
The 1B parameter variant of Gemma 3 is lightweight and runs well on 16GB GPUs. For larger Gemma models, you'll need GPUs with more VRAM.
</Note>

Your endpoint will begin initializing. This may take several minutes while Runpod provisions resources and downloads the model.

## Step 3: Note your endpoint ID

Once your endpoint is deployed, make a note of your **Endpoint ID** from the endpoint details page. You'll need this to construct your API base URL.

<Frame>
  <img src="/images/4a0706af-serverless-endpoint-id.png" />
</Frame>

Your API base URL will follow this pattern:

```
https://api.runpod.ai/v2/{ENDPOINT_ID}/openai/v1
```

## Step 4: Test your endpoint

Before building a chatbot, verify that your endpoint is working correctly using a simple test request.

Set up your environment variables:

```bash
export RUNPOD_API_KEY="your_runpod_api_key"
export RUNPOD_ENDPOINT_ID="your_endpoint_id"
```

Send a test request using `curl`:

```bash
curl "https://api.runpod.ai/v2/${RUNPOD_ENDPOINT_ID}/openai/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer ${RUNPOD_API_KEY}" \
  -d '{
    "model": "google/gemma-3-1b-it",
    "messages": [
      {"role": "user", "content": "Hello! What is your name?"}
    ],
    "max_tokens": 100
  }'
```

You should receive a JSON response containing the model's reply. If the request fails, check that:

- Your endpoint has finished initializing (status shows "Ready").
- Your `HF_TOKEN` environment variable is set correctly on the endpoint.
- You've accepted Google's terms on Hugging Face.

## Step 5: Build a chatbot

With your endpoint deployed, you can interact with it using the OpenAI-compatible API. The following example creates a command-line chatbot that maintains conversation history and generates responses using your deployed model.

Create a file called `gemma_chat.py`:

```python
from openai import OpenAI
import os

# Initialize the client with your Runpod endpoint
client = OpenAI(
    base_url=f"https://api.runpod.ai/v2/{os.environ['RUNPOD_ENDPOINT_ID']}/openai/v1",
    api_key=os.environ["RUNPOD_API_KEY"],
)

# Initialize conversation history
messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant.",
    }
]


def display_chat_history(messages):
    """Display the conversation history."""
    for message in messages:
        if message["role"] != "system":
            print(f"{message['role'].capitalize()}: {message['content']}")


def get_assistant_response(messages):
    """Get a response from the Gemma model."""
    response = client.chat.completions.create(
        model="google/gemma-3-1b-it",
        messages=messages,
        temperature=0.7,
        top_p=0.9,
        max_tokens=256,
    )
    return response.choices[0].message.content


def main():
    print("Gemma 3 Chatbot")
    print("Type 'quit' to exit.\n")

    while True:
        # Get user input
        user_input = input("You: ")

        if user_input.lower() == "quit":
            print("Goodbye!")
            break

        # Add user message to history
        messages.append({"role": "user", "content": user_input})

        # Get and display response
        response = get_assistant_response(messages)
        messages.append({"role": "assistant", "content": response})

        print(f"Assistant: {response}\n")


if __name__ == "__main__":
    main()
```

Run your chatbot:

```bash
python gemma_chat.py
```

You can now have a conversation with Gemma 3:

```
Gemma 3 Chatbot
Type 'quit' to exit.

You: What are some fun facts about space?
Assistant: Here are some fun facts about space:

1. A day on Venus is longer than its year - Venus takes 243 Earth days to rotate once but only 225 Earth days to orbit the Sun.

2. There's a planet made of diamonds - 55 Cancri e is believed to be covered in graphite and diamond.

3. Space is completely silent - with no atmosphere to carry sound waves, space is eerily quiet.

You: quit
Goodbye!
```

## Next steps

You've successfully deployed Gemma 3 on Runpod Serverless and built a chatbot to interact with it. Here are some ways to extend this tutorial:

- [Configure your endpoint](/serverless/endpoints/endpoint-configurations) to optimize performance and cost.
- [Learn about vLLM environment variables](/serverless/vllm/environment-variables) to customize model behavior.
- [Explore OpenAI compatibility](/serverless/vllm/openai-compatibility) for features like streaming and function calling.
- [Build a custom worker](/serverless/workers/custom-worker) for more specialized use cases.
