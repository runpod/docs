---
title: "Deploy Qwen2.5 with Cached Models"
sidebarTitle: "Qwen2.5 Cached Model"
description: "Learn how to create a custom serverless endpoint that uses cached models to serve the Qwen2.5 LLM with minimal cold start times."
tag: "NEW"
---

This tutorial demonstrates how to build a custom serverless worker that leverages Runpod's [cached models](/serverless/endpoints/model-caching) feature to serve the Qwen2.5 language model. You'll learn how to create a custom handler function that programmatically locates and loads cached models, dramatically reducing cold start times.

## What you'll learn

- How to configure an endpoint with cached models
- How to programmatically locate cached models in your worker
- How to create a custom handler function for text generation
- How to integrate the Qwen2.5 model with the Hugging Face Transformers library

## Prerequisites

Before starting, ensure you have:

- A [Runpod account](https://www.runpod.io/) with API access
- [Docker](https://www.docker.com/get-started) installed (for building custom workers)
- (Optional) A [Hugging Face access token](https://huggingface.co/settings/tokens) if using gated models

## Step 1: Create your handler function

First, create a `handler.py` file that will process inference requests and use the cached model. This handler includes a helper function to locate the cached model and implements the inference logic.

```python handler.py
import os
import runpod
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Cache directory where Runpod stores cached models
CACHE_DIR = "/runpod-volume/huggingface-cache/hub"

def find_model_path(model_name):
    """
    Find the path to a cached model.

    Args:
        model_name: The model name from Hugging Face
        (e.g., 'Qwen/Qwen2.5-0.5B-Instruct')

    Returns:
        The full path to the cached model, or None if not found
    """
    # Convert model name format: "Org/Model" -> "models--Org--Model"
    cache_name = model_name.replace("/", "--")
    snapshots_dir = os.path.join(CACHE_DIR, f"models--{cache_name}", "snapshots")

    # Check if the model exists in cache
    if os.path.exists(snapshots_dir):
        snapshots = os.listdir(snapshots_dir)
        if snapshots:
            # Return the path to the first (usually only) snapshot
            return os.path.join(snapshots_dir, snapshots[0])

    return None

# Initialize model and tokenizer as global variables
MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"
model = None
tokenizer = None

def load_model():
    """
    Load the model and tokenizer from the cached location.
    This runs once when the worker starts.
    """
    global model, tokenizer

    print(f"Loading model: {MODEL_NAME}")

    # Find the cached model path
    model_path = find_model_path(MODEL_NAME)

    if model_path:
        print(f"Found cached model at: {model_path}")
    else:
        print(f"Cached model not found, will download from Hugging Face")
        model_path = MODEL_NAME

    # Load tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto"
    )

    print("Model loaded successfully!")

def handler(job):
    """
    Handler function that processes each inference request.

    Args:
        job: Runpod job object containing input data

    Returns:
        Dictionary with generated text
    """
    job_input = job["input"]

    # Extract parameters from the request
    prompt = job_input.get("prompt", "Hello! How are you?")
    max_tokens = job_input.get("max_tokens", 100)
    temperature = job_input.get("temperature", 0.7)

    # Prepare the input
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # Generate response
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=temperature,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )

    # Decode the generated text
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return {
        "generated_text": generated_text,
        "prompt": prompt,
        "model": MODEL_NAME
    }

if __name__ == "__main__":
    # Load the model when the worker starts
    load_model()

    # Start the Runpod serverless worker
    runpod.serverless.start({"handler": handler})
```

<Tip>
The `find_model_path()` function automatically locates the cached model by converting the Hugging Face model name format to Runpod's cache directory structure. This eliminates the need to hardcode version hashes.
</Tip>

## Step 2: Create a Dockerfile

Create a `Dockerfile` to package your handler into a container image:

```dockerfile Dockerfile
FROM runpod/base:0.6.2-cuda12.2.0

# Set working directory
WORKDIR /app

# Install Python dependencies
RUN pip install --no-cache-dir \
    runpod \
    transformers \
    torch \
    accelerate

# Copy handler file
COPY handler.py /app/handler.py

# Set the handler as the entry point
CMD ["python3", "-u", "handler.py"]
```

## Step 3: Set up your GitHub repository

Create a GitHub repository with your handler and Dockerfile:

1. Create a new repository on GitHub (e.g., `qwen-cached-worker`)

2. Add your files to the repository:

```bash
# Initialize git repository
git init
git add handler.py Dockerfile
git commit -m "Initial commit: Qwen2.5 cached model worker"

# Push to GitHub
git remote add origin https://github.com/YOUR_USERNAME/qwen-cached-worker.git
git branch -M main
git push -u origin main
```

Replace `YOUR_USERNAME` with your actual GitHub username.

## Step 4: Connect Runpod to GitHub

Before deploying, authorize Runpod to access your repositories:

1. Open the [settings page](http://console.runpod.io/user/settings) in the Runpod console

2. Under **Connections**, find the **GitHub** card and click **Connect**

3. Sign in and authorize Runpod to access your repositories:
   - Choose **All repositories** or **Only select repositories**
   - Select your `qwen-cached-worker` repository if using selective access

4. Click **Save**

## Step 5: Deploy from GitHub with cached models

Now deploy your worker directly from GitHub:

1. Navigate to the [Serverless section](https://www.console.runpod.io/serverless) and click **New Endpoint**

2. Under **Import Git Repository**, select your `qwen-cached-worker` repository

3. Configure deployment options:
   - **Branch**: Select `main` (or your preferred branch)
   - **Dockerfile Path**: Leave as default if Dockerfile is in the root
   - Click **Next**

4. Configure endpoint settings:
   - **Endpoint Name**: Choose a descriptive name (e.g., "qwen-cached-inference")
   - **Endpoint Type**: Select **Queue** for traditional processing
   - **GPU Configuration**: Select a GPU (e.g., RTX 4090 or A40)
   - **Workers**: Set minimum workers to 0, maximum to 3
   - **Container Disk**: Allocate at least 10 GB

5. **Enable Cached Models**:
   - Scroll to the **Model** section
   - Enter the model name: `Qwen/Qwen2.5-0.5B-Instruct`
   - (Optional) If using a gated model, add your Hugging Face token

6. Click **Deploy Endpoint**

Runpod will automatically build your Docker image and deploy it to your endpoint. You can monitor the build status in the **Builds** tab.

<Frame alt="Cached model configuration">
  <img src="/images/model-cache-setting.png" />
</Frame>

## Step 6: Test your endpoint

Once deployed, you can send requests to your endpoint using the Runpod API:

<Tabs>
  <Tab title="Python">
```python
import requests
import os

# Your endpoint details
endpoint_id = "YOUR_ENDPOINT_ID"
api_key = os.environ.get("RUNPOD_API_KEY")

# Prepare the request
url = f"https://api.runpod.ai/v2/{endpoint_id}/runsync"
headers = {
    "Authorization": f"Bearer {api_key}",
    "Content-Type": "application/json"
}

payload = {
    "input": {
        "prompt": "Explain what large language models are in simple terms.",
        "max_tokens": 150,
        "temperature": 0.7
    }
}

# Send the request
response = requests.post(url, json=payload, headers=headers)
result = response.json()

print("Generated text:", result["output"]["generated_text"])
```
  </Tab>
  <Tab title="cURL">
```bash
curl -X POST https://api.runpod.ai/v2/YOUR_ENDPOINT_ID/runsync \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "input": {
      "prompt": "Explain what large language models are in simple terms.",
      "max_tokens": 150,
      "temperature": 0.7
    }
  }'
```
  </Tab>
</Tabs>

Expected response:

```json
{
  "id": "sync-request-id",
  "status": "COMPLETED",
  "output": {
    "generated_text": "Explain what large language models are in simple terms. Large language models (LLMs) are AI systems trained on vast amounts of text data...",
    "prompt": "Explain what large language models are in simple terms.",
    "model": "Qwen/Qwen2.5-0.5B-Instruct"
  }
}
```

## Update your worker

When you make changes to your handler or Dockerfile, you can update your endpoint by creating a new GitHub release:

1. Make your code changes and push to GitHub:

```bash
git add .
git commit -m "Update handler with new features"
git push origin main
```

2. Create a new release in your GitHub repository:
   - Go to your repository on GitHub
   - Click **Releases** â†’ **Create a new release**
   - Tag the release (e.g., `v1.0.1`)
   - Click **Publish release**

3. Runpod will automatically detect the new release and rebuild your worker

<Note>
You can monitor the build progress in the **Builds** tab of your endpoint. If needed, you can roll back to any previous build directly from the Runpod console.
</Note>

## Benefits of using cached models

By using Runpod's cached model feature in this tutorial, you gain several advantages:

- **Faster cold starts**: Workers start in seconds instead of minutes
- **Cost savings**: No billing during model download time
- **Simplified deployment**: Models are automatically available to all workers
- **Better scalability**: Quick worker scaling without waiting for downloads

## Next steps

Now that you have a working Qwen2.5 endpoint with cached models, you can:

- Experiment with different [Qwen model variants](https://huggingface.co/Qwen) (Qwen2.5-1.5B, Qwen2.5-7B, etc.)
- Add more sophisticated prompt templates and chat formatting
- Implement streaming responses for real-time generation
- Integrate with existing applications using the OpenAI-compatible API

## Related resources

<CardGroup cols={2}>
  <Card title="Cached Models" icon="bolt" href="/serverless/endpoints/model-caching">
    Learn more about cached models and their benefits
  </Card>
  <Card title="GitHub Integration" icon="github" href="/serverless/workers/github-integration">
    Deploy workers directly from GitHub repositories
  </Card>
  <Card title="Handler Functions" icon="function" href="/serverless/workers/handler-functions">
    Understand handler function structure and best practices
  </Card>
  <Card title="vLLM Workers" icon="rocket" href="/serverless/vllm/overview">
    Explore vLLM for optimized LLM inference
  </Card>
</CardGroup>
