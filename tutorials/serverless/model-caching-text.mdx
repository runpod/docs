---
title: "Deploy Phi-3 using model caching"
sidebarTitle: "Deploy a cached model"
description: "Learn how to create a custom Serverless endpoint that uses model caching to serve Phi-3 with reduced cost and cold start times."
tag: "NEW"
---

<Tip>
You can download the finished code for this tutorial [on GitHub](https://github.com/stuffbyt/model-store-worker).
</Tip>

This tutorial demonstrates how to build a custom Serverless worker that leverages Runpod's [cached model](/serverless/endpoints/model-caching) feature to serve the Phi-3 language model. You'll learn how to create a handler function that locates and loads cached models in offline mode, which can significantly reduce costs and cold start times.

## What you'll learn

- How to configure a Serverless endpoint with a cached model.
- How to programmatically locate a cached model in your handler function.
- How to create a custom handler function for text generation.
- How to integrate the Phi-3 model with the Hugging Face Transformers library.

## Requirements

Before starting this tutorial, make sure:

- You have a [Runpod account](/get-started/manage-accounts) with sufficient credits.
- You have a [Runpod API key](/get-started/api-keys).
- You have a [GitHub account](https://github.com/). 
- Your Runpod account is [connected to GitHub](/serverless/workers/github-integration#authorize-runpod-with-github).


## Understanding the cached model file structure

Runpod caches models at `/runpod-volume/huggingface-cache/hub/`.

The directory structure for cached models follows this pattern:

```
/runpod-volume/huggingface-cache/hub/models--{org}--{model-name}/snapshots/{version-hash}/
```

For example, `microsoft/Phi-3-mini-4k-instruct` becomes:

```
/runpod-volume/huggingface-cache/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/{hash}/
```

## Step 1: Create project files

Create a new directory for your project:

```bash
mkdir runpod-model-store-worker
cd runpod-model-store-worker
touch Dockerfile requirements.txt handler.py
```

Your project structure should now look like this:

```
runpod-model-store-worker/
├── Dockerfile
├── requirements.txt
└── handler.py
```

## Step 2: Create requirements.txt

Add the following dependencies to `requirements.txt`:

```txt
runpod>=1.6.2
transformers>=4.38.0
torch>=2.1.0
accelerate>=0.27.0
```

## Step 3: Create your handler function

Open `handler.py` and add the following code:

```python handler.py
import runpod
import torch
import os
from transformers import pipeline

# Configure Hugging Face to use Runpod's cache directory.
# These must be set before loading any models.
os.environ['HF_HOME'] = '/runpod-volume/huggingface-cache'
os.environ['TRANSFORMERS_CACHE'] = '/runpod-volume/huggingface-cache'
os.environ['HF_HUB_CACHE'] = '/runpod-volume/huggingface-cache/hub'

# Model configuration (can be overridden via environment variables)
MODEL_NAME = os.environ.get('MODEL_NAME', 'microsoft/Phi-3-mini-4k-instruct')
MODEL_REVISION = os.environ.get('MODEL_REVISION', 'main')

print(f"Loading model: {MODEL_NAME} (revision: {MODEL_REVISION})")
print(f"Cache directory: {os.environ['HF_HOME']}")

# Verify the model exists in cache before loading
model_cache_dir = f"models--{MODEL_NAME.replace('/', '--')}"
model_cache_path = os.path.join(os.environ['HF_HOME'], 'hub', model_cache_dir, 'snapshots')

if os.path.exists(model_cache_path) and os.listdir(model_cache_path):
    print(f"Model found in cache: {model_cache_path}")
else:
    print(f"Model not in cache, will download from Hugging Face")

# Load the model into a text-generation pipeline.
# This runs once at startup; the model stays in GPU memory for all requests.
pipe = pipeline(
    "text-generation",
    model=MODEL_NAME,
    revision=MODEL_REVISION,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    model_kwargs={"cache_dir": "/runpod-volume/huggingface-cache"},
)
print("Model loaded successfully")


def handler(event):
    """Process a text generation request."""
    try:
        # Extract request parameters
        input_data = event["input"]
        user_prompt = input_data.get("prompt", "Hello!")
        max_tokens = input_data.get("max_tokens", 256)
        temperature = input_data.get("temperature", 0.7)

        # Format the prompt using the model's chat template
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": user_prompt},
        ]
        formatted_prompt = pipe.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

        # Generate a response
        outputs = pipe(
            formatted_prompt,
            max_new_tokens=max_tokens,
            do_sample=True,
            temperature=temperature,
            top_k=50,
            top_p=0.95,
        )

        return {"output": outputs[0]["generated_text"], "status": "success"}

    except Exception as e:
        return {"error": str(e), "status": "error"}


if __name__ == "__main__":
    runpod.serverless.start({"handler": handler})
```

### Understanding the handler

If you want to learn more about each component of the handler function, expand the following section:

<Accordion title="Handler function details">

The handler is divided into four main sections: cache configuration, model loading, chat formatting, and request handling.

#### Cache configuration

```python
os.environ['HF_HOME'] = '/runpod-volume/huggingface-cache'
os.environ['TRANSFORMERS_CACHE'] = '/runpod-volume/huggingface-cache'
os.environ['HF_HUB_CACHE'] = '/runpod-volume/huggingface-cache/hub'

MODEL_NAME = os.environ.get('MODEL_NAME', 'microsoft/Phi-3-mini-4k-instruct')
MODEL_REVISION = os.environ.get('MODEL_REVISION', 'main')
```

The handler starts by configuring Hugging Face's cache directories to point to Runpod's cache location. When you enable model caching on your endpoint, Runpod pre-downloads the model to `/runpod-volume/huggingface-cache/` before your worker starts. By setting these environment variables, the Transformers library automatically finds and uses the cached model files.

`MODEL_NAME` specifies which Hugging Face model to load. You can override this default by setting the `MODEL_NAME` environment variable in your endpoint configuration, which lets you reuse the same handler code with different models. `MODEL_REVISION` controls which version of the model to load (defaults to `main`).

#### Cache verification

```python
model_cache_dir = f"models--{MODEL_NAME.replace('/', '--')}"
model_cache_path = os.path.join(os.environ['HF_HOME'], 'hub', model_cache_dir, 'snapshots')

if os.path.exists(model_cache_path) and os.listdir(model_cache_path):
    print(f"Model found in cache: {model_cache_path}")
else:
    print(f"Model not in cache, will download from Hugging Face")
```

Before loading the model, the handler checks whether it exists in the cache. Hugging Face stores cached models using a specific directory structure where `microsoft/Phi-3-mini-4k-instruct` becomes `models--microsoft--Phi-3-mini-4k-instruct`. This check provides early feedback in your worker logs about whether caching is working correctly.

#### Model loading

```python
pipe = pipeline(
    "text-generation",
    model=MODEL_NAME,
    revision=MODEL_REVISION,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    model_kwargs={"cache_dir": "/runpod-volume/huggingface-cache"},
)
```

Model loading happens at the module level, outside any function. This means it runs once when the worker starts, not on every request. The model stays in GPU memory and gets reused across all incoming jobs, which is essential for performance.

The `pipeline()` function creates a high-level interface that handles tokenization, generation, and decoding in a single call. Key parameters:

- `torch_dtype=torch.bfloat16` uses 16-bit floating point precision to reduce memory usage while maintaining model quality.
- `device_map="auto"` lets the Accelerate library automatically place model layers across available GPUs.
- `cache_dir` explicitly points to the Runpod cache location as an additional safeguard.

#### Chat formatting

```python
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": user_prompt},
]
formatted_prompt = pipe.tokenizer.apply_chat_template(
    messages, tokenize=False, add_generation_prompt=True
)
```

Modern chat models like Phi-3 expect prompts in a specific format with role markers and special tokens. The `apply_chat_template()` method converts a list of message dictionaries into the exact format the model was trained on. This is important because different models use different chat formats; using the tokenizer's built-in template ensures compatibility.

Setting `add_generation_prompt=True` appends the assistant's turn marker, signaling to the model that it should generate a response.

#### Request handling

```python
def handler(event):
    input_data = event["input"]
    user_prompt = input_data.get("prompt", "Hello!")
    max_tokens = input_data.get("max_tokens", 256)
    temperature = input_data.get("temperature", 0.7)
```

The `handler` function processes each incoming request. The `event` parameter is a dictionary containing the request data, with user inputs nested under the `"input"` key. The handler extracts parameters with sensible defaults.

```python
    outputs = pipe(
        formatted_prompt,
        max_new_tokens=max_tokens,
        do_sample=True,
        temperature=temperature,
        top_k=50,
        top_p=0.95,
    )
    return {"output": outputs[0]["generated_text"], "status": "success"}
```

The generation parameters control the model's output:

- `max_new_tokens` limits the response length.
- `do_sample=True` enables probabilistic sampling (required for temperature to have an effect).
- `temperature` controls randomness (higher = more creative, lower = more deterministic).
- `top_k` and `top_p` provide additional control over the sampling distribution.

The pipeline returns a list of dictionaries. Since we're processing a single prompt, we take `outputs[0]["generated_text"]` and return it in a structured response.

```python
runpod.serverless.start({"handler": handler})
```

The final line registers the handler function with the Runpod SDK and starts the worker's event loop, which polls for jobs and dispatches them to your handler.

</Accordion>

## Step 4: Create a Dockerfile

Create a `Dockerfile` to package your handler into a container image.

```dockerfile Dockerfile

# Use Runpod's PyTorch base image
FROM runpod/pytorch:2.1.1-py3.10-cuda12.1.1-devel-ubuntu22.04

# Set working directory
WORKDIR /app

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy the handler function to the container
COPY handler.py .

# Set the handler as the entry point for the container
CMD ["python", "-u", "handler.py"]
```

## Step 5: Set up your GitHub repository

Create a GitHub repository with your handler, requirements, and Dockerfile.

1. Create a new repository on GitHub (for example, `phi3-cached-worker`).

2. Add your files to the repository:

```bash
git init
git add handler.py requirements.txt Dockerfile
git commit -m "Initial commit: Phi-3 cached model worker"

git remote add origin https://github.com/YOUR_USERNAME/phi3-cached-worker.git
git branch -M main
git push -u origin main
```

Replace `YOUR_USERNAME` with your GitHub username.

## Step 6: Deploy from GitHub

Deploy your worker directly from GitHub.

1. Navigate to the [Serverless section](https://www.console.runpod.io/serverless) and select **New Endpoint**.

2. Under **Import Git Repository**, select your `phi3-cached-worker` repository.

3. Configure deployment options:
   - **Branch**: Select `main` (or your preferred branch).
   - **Dockerfile Path**: Leave as default if Dockerfile is in the root.
   - Select **Next**.

4. Configure endpoint settings:
   - **Endpoint Name**: Choose a descriptive name (for example, "phi3-cached-inference").
   - **Endpoint Type**: Make sure it's set to **Queue**.
   - **GPU Configuration**: Select one or more GPU types with at least 16GB VRAM.
   - **Workers**: Leave the defaults in place (minimum: 0, maximum: 3).
   - **Container Disk**: Allocate at least 20 GB (or more if you're using a larger model).

5. **Enable cached models**:
   - Scroll to the **Model** section.
   - Enter the model name: 
      ```text
      microsoft/Phi-3-mini-4k-instruct
      ```
      ... or your preferred model that's available on Hugging Face.
   - (Optional) If using a gated model, add your Hugging Face token.

6. Select **Deploy Endpoint**.

Runpod automatically builds your Docker image and deploys it to your endpoint. You can monitor the build status in the **Builds** tab.

## Step 7: Test your endpoint

Once deployed, send requests to your endpoint using the Runpod API. Replace `YOUR_ENDPOINT_ID` with your actual endpoint ID.

<Tabs>
  <Tab title="Python">
```python request.py
import requests
import os

endpoint_id = "YOUR_ENDPOINT_ID"
api_key = os.environ.get("RUNPOD_API_KEY")

url = f"https://api.runpod.ai/v2/{endpoint_id}/runsync"
headers = {
    "Authorization": f"Bearer {api_key}",
    "Content-Type": "application/json",
}

payload = {
    "input": {
        "prompt": "Explain what large language models are in simple terms.",
        "max_tokens": 150,
        "temperature": 0.7,
    }
}

response = requests.post(url, json=payload, headers=headers)
result = response.json()

print("Generated text:", result["output"]["output"])
```
  </Tab>
  <Tab title="cURL">
```bash
curl -X POST https://api.runpod.ai/v2/YOUR_ENDPOINT_ID/runsync \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "input": {
      "prompt": "Explain what large language models are in simple terms.",
      "max_tokens": 150,
      "temperature": 0.7
    }
  }'
```
  </Tab>
</Tabs>

Expected response:

```json
{
  "id": "sync-request-id",
  "status": "COMPLETED",
  "output": {
    "status": "success",
    "output": "Explain what large language models are in simple terms. Large language models (LLMs) are AI systems trained on vast amounts of text data..."
  }
}
```

Congratulations! You've successfully deployed a Serverless endpoint that uses model caching to serve Phi-3.

## Benefits of using cached models

By using Runpod's cached model feature in this tutorial, you gain several advantages:

- **Faster cold starts**: Workers start in seconds instead of minutes.
- **Cost savings**: No billing during model download time.
- **Simplified deployment**: Models are automatically available to all workers.
- **Better scalability**: Quick worker scaling without waiting for downloads.

## Next steps

Now that you have a working Phi-3 endpoint with cached models, you can:

- Experiment with different [Phi model variants](https://huggingface.co/microsoft) (Phi-3-medium, Phi-3.5, etc.).
- Add more sophisticated prompt templates and chat formatting.
- Implement streaming responses for real-time generation.
- Integrate with existing applications using the Runpod SDK.

## Related resources

<CardGroup cols={2}>
  <Card title="Cached models" icon="bolt" href="/serverless/endpoints/model-caching">
    Learn more about cached models and their benefits
  </Card>
  <Card title="GitHub integration" icon="github" href="/serverless/workers/github-integration">
    Deploy workers directly from GitHub repositories
  </Card>
  <Card title="Handler functions" icon="function" href="/serverless/workers/handler-functions">
    Understand handler function structure and best practices
  </Card>
  <Card title="vLLM workers" icon="rocket" href="/serverless/vllm/overview">
    Explore vLLM for optimized LLM inference
  </Card>
</CardGroup>
