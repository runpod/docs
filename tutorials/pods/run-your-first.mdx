---
title: "Run LLMs with JupyterLab using the transformers library"
sidebarTitle: "Run LLMs with JupyterLab"
description: "Learn how to run the SmolLM3 model in JupyterLab using the transformers library."
---

This tutorial walks you through how to deploy a Pod and use the transformers library to run SmolLM3, a small but powerful language model, using JupyterLab for interactive development.

[SmolLM3](https://huggingface.co/docs/transformers/en/model_doc/smollm3) is a family of small language models developed by Hugging Face that provides strong performance while being efficient enough to run on modest hardware. 
The 3B parameter model we'll use in this tutorial requires only 24 GB of VRAM, making it accessible for experimentation and development.

## What you'll learn

In this tutorial, you'll learn how to:

- Deploy a Pod with the PyTorch template.
- Access the web terminal and JupyterLab services.
- Install the transformers and accelerate libraries.
- Use SmolLM3 for text generation in a Python notebook.
- Configure model parameters for different use cases.

## Requirements

Before you begin, you'll need:

- A [Runpod account](/get-started/manage-accounts).
- At least $5 in Runpod credits.
- Basic familiarity with Python and Jupyter notebooks.

## Step 1: Deploy a Pod with PyTorch template

First, you'll deploy a Pod using the default PyTorch template, which comes with PyTorch 2.4 or higher pre-installed:

1. Navigate to the [Pods page](https://console.runpod.io/pods) in the Runpod console.
2. Click **Deploy** to create a new Pod.
3. In the template selection, choose the **Runpod PyTorch** template (this should be the default option).
4. For GPU selection, choose any GPU with 24 GB or more VRAM. Good options include:
   - RTX 4090 (24 GB VRAM)
   - RTX A5000 (24 GB VRAM) 
   - L40 (48 GB VRAM)
5. Keep the default storage settings, which should be sufficient for this tutorial.
6. Click **Deploy On-Demand** to create your Pod.

Wait for your Pod to initialize. This typically takes 2-5 minutes.

## Step 2: Install required packages

Once your Pod is running, you'll need to install the transformers and accelerate libraries:

1. In the Runpod console, find your deployed Pod and click **Connect**.
2. Click **Start Web Terminal** to open a terminal session in your browser.
3. In the terminal, install the required packages by running:

```bash
pip install transformers accelerate
```

This command installs both the transformers library (for working with SmolLM3) and the accelerate library (for optimized model loading and inference).

## Step 3: Open JupyterLab

Now you'll access JupyterLab to run your Python code interactively:

1. Go back to the Runpod console and click **Connect** on your Pod again.
2. Under **HTTP Services**, click **Connect to HTTP Service [Port 8888]** to open JupyterLab.
3. If the JupyterLab service shows as "Not Ready", wait a moment and refresh the page.

JupyterLab will open in a new browser tab, providing you with an interactive Python environment.

## Step 4: Create and run your SmolLM3 notebook

In JupyterLab, you'll create a new notebook and run SmolLM3:

1. In JupyterLab, click **File** > **New** > **Notebook**.
2. Select **Python 3 (ipykernel)** when prompted for the kernel.
3. In the first cell of your notebook, enter the following code:

```python
import torch
from transformers import pipeline

# Create a text generation pipeline with SmolLM3
pipe = pipeline(
    task="text-generation",
    model="HuggingFaceTB/SmolLM3-3B",
    torch_dtype=torch.bfloat16,
    device_map=0
)

# Define a conversation with system and user messages
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Tell me about yourself."},
]

# Generate text with the model
outputs = pipe(messages, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)

# Print the generated response
print(outputs[0]["generated_text"][-1]['content'])
```

4. Run the cell by pressing **Shift + Enter** or clicking the **Run** button.

The first time you run this code, it will download the SmolLM3 model (approximately 6 GB), which may take several minutes depending on your internet connection. Subsequent runs will be much faster as the model will be cached locally.

## Step 5: Experiment with different prompts and parameters

Once your model is loaded, you can experiment with different prompts and generation parameters:

### Try different conversation topics

```python
# Example: Ask for creative writing
messages = [
    {"role": "system", "content": "You are a creative writing assistant."},
    {"role": "user", "content": "Write a short story about a robot discovering emotions."},
]

outputs = pipe(messages, max_new_tokens=300, do_sample=True, temperature=0.8)
print(outputs[0]["generated_text"][-1]['content'])
```

### Adjust generation parameters

You can modify various parameters to control the model's output:

- **max_new_tokens**: Controls the maximum length of the generated text
- **temperature**: Controls randomness (0.1 = more focused, 1.0 = more creative)
- **top_k**: Limits the vocabulary to the top K most likely tokens
- **top_p**: Uses nucleus sampling to control diversity

```python
# More focused and deterministic output
outputs = pipe(messages, max_new_tokens=150, do_sample=True, temperature=0.3, top_p=0.9)
print(outputs[0]["generated_text"][-1]['content'])
```

### Use single-turn prompts

You can also use SmolLM3 for simple text completion without the chat format:

```python
# Simple text completion
prompt = "The future of artificial intelligence is"
outputs = pipe(prompt, max_new_tokens=200, do_sample=True, temperature=0.7)
print(outputs[0]["generated_text"])
```

## Understanding the code

Let's break down the key components of the SmolLM3 implementation:

- **pipeline()**: Creates a high-level interface for text generation
- **torch_dtype=torch.bfloat16**: Uses 16-bit floating point for memory efficiency
- **device_map=0**: Automatically places the model on the first available GPU
- **messages format**: Uses a chat-like structure with system and user roles
- **Generation parameters**: Control the creativity and length of outputs

For more detailed information about SmolLM3's capabilities and parameters, see the [official SmolLM3 documentation](https://huggingface.co/docs/transformers/en/model_doc/smollm3?usage=Pipeline#transformers.SmolLM3Model).

## What's next

Now that you have SmolLM3 running, you can explore more advanced use cases:

### Fine-tuning

Consider fine-tuning SmolLM3 on your specific dataset using the [Axolotl fine-tuning tutorial](/tutorials/pods/fine-tune-llm-axolotl).

### Integration with applications

Use SmolLM3 as part of larger applications by integrating it with web frameworks or APIs.

### Model comparison

Try other models in the SmolLM3 family or compare with other small language models to find the best fit for your use case.

### Persistent storage

If you plan to work with SmolLM3 regularly, consider using a [network volume](/pods/storage/create-network-volumes) to persist your models and notebooks across Pod sessions.

## Troubleshooting

Here are solutions to common issues:

- **Out of memory errors**: Ensure you're using a GPU with at least 24 GB VRAM, or try reducing the batch size
- **Model download fails**: Check your internet connection and try running the cell again
- **JupyterLab not accessible**: Wait a few minutes after Pod deployment for services to fully start
- **Import errors**: Make sure you installed the packages in Step 2 using the web terminal

Remember to stop your Pod when you're finished to avoid unnecessary charges. You can always restart it later to continue your work.