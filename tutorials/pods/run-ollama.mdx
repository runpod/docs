---
title: "Set up Ollama on a GPU Pod"
sidebarTitle: "Set up Ollama"
description: "Learn how to install and run Ollama on a GPU Pod with HTTP API access."
---

This tutorial shows you how to set up [Ollama](https://ollama.com), a platform for running large language models, on a Runpod GPU Pod. By the end, you'll have Ollama running with HTTP API access for external requests.

## What you'll learn

- Deploy a Pod with the PyTorch template.
- Install and configure Ollama for external access.
- Run AI models and interact via CLI or HTTP API.

## Requirements

- A Runpod account with credits.

## Step 1: Deploy a Pod

1. Log in to the [Runpod console](https://www.console.runpod.io/pods) and select **+ GPU Pod**.
2. Choose a GPU (for example, A40).
3. Select the latest **PyTorch** template.
4. Select **Customize Deployment**:
   - Add port `11434` to the exposed ports list.
   - Add an environment variable with key `OLLAMA_HOST` and value `0.0.0.0`.
5. Select **Set Overrides**, then **Continue**, then **Deploy**.

The `OLLAMA_HOST` setting allows Ollama to accept requests from external clients through the exposed port.

## Step 2: Install Ollama

1. Once the Pod is running, select **Connect** > **Start Web Terminal** > **Connect to Web Terminal**.
2. Enter your username and password.
3. Update packages and install dependencies:

   ```bash
   apt update && apt install -y lshw
   ```

4. Install Ollama and start the server in the background:

   ```bash
   (curl -fsSL https://ollama.com/install.sh | sh && ollama serve > ollama.log 2>&1) &
   ```

## Step 3: Run a model

Download and run a model using the `ollama run` command:

```bash
ollama run llama2
```

Replace `llama2` with any model from the [Ollama library](https://ollama.com/library). You can now interact with the model directly from the terminal.

## Step 4: Make HTTP API requests

With Ollama running, you can make HTTP requests to your Pod from external clients.

**List available models:**

```bash
curl https://{POD_ID}-11434.proxy.runpod.net/api/tags
```

**Generate a response:**

```bash
curl -X POST https://{POD_ID}-11434.proxy.runpod.net/api/generate -d '{
  "model": "llama2",
  "prompt": "Tell me a story about llamas"
}'
```

Replace `{POD_ID}` with your actual Pod ID.

For more API options, see the [Ollama API documentation](https://github.com/ollama/ollama/blob/main/docs/api.md).

## Next steps

- Learn about [exposing ports](/pods/configuration/expose-ports) on Pods.
- Connect [VSCode to Runpod](https://blog.runpod.io/how-to-connect-vscode-to-runpod/) for remote development.
- Explore more models in the [Ollama library](https://ollama.com/library).
