---
sidebarTitle: n8n
title: "Integrate Runpod with n8n"
description: "Learn how to deploy a vLLM worker on Runpod and connect it to n8n for AI-powered workflow automation."
tag: "NEW"
---

Learn how to integrate Runpod Serverless with n8n, a workflow automation tool. By the end of this tutorial, you'll have a vLLM endpoint running on Runpod that you can use within your n8n workflows.

## What you'll learn

In this tutorial, you'll learn how to:

* Deploy a vLLM worker on Runpod Serverless.
* Configure your vLLM endpoint for OpenAI compatibility.
* Connect n8n to your Runpod endpoint.
* Test your integration with a simple workflow.

## Requirements

* You've [created a Runpod account](/get-started/manage-accounts).
* You've created a [Runpod API key](/get-started/api-keys).
* You have [n8n](https://n8n.io/) installed and running.
* (Optional) For gated models, you've created a [Hugging Face access token](https://huggingface.co/docs/hub/en/security-tokens).

## Step 1: Deploy a vLLM worker on Runpod

First, you'll deploy a vLLM worker to serve your language model.

<Steps>
  <Step title="Create a new vLLM endpoint">
    Open the [Runpod console](https://www.console.runpod.io/serverless) and navigate to the Serverless page.
    
    Click **New Endpoint** and select **vLLM** under **Ready-to-Deploy Repos**.

  </Step>

  <Step title="Configure your endpoint">
    <Tip>
    For more details on vLLM deployment options, see [Deploy a vLLM worker](/serverless/vllm/get-started).
    </Tip>

    In the deployment modal:

    * Enter the model name or Hugging Face model URL (e.g., `openchat/openchat-3.5-0106`).
    * Expand the **Advanced** section:
      * Set **Max Model Length** to `8192` (or an appropriate context length for your model).
      * You may need to enable tool calling and set an appropriate reasoning parser depending on your model.
    * Click **Next**.
    * Click **Create Endpoint**.

    Your endpoint will now begin initializing. This may take several minutes while Runpod provisions resources and downloads your model. Wait until the status shows as **Running**.
  </Step>

  <Step title="Copy your endpoint ID">
    Once deployed, navigate to your endpoint in the Runpod console and copy the **Endpoint ID**. You'll need this to connect your endpoint to n8n.
  </Step>
</Steps>

## Step 2: Connect n8n to your Runpod endpoint

Now you'll configure n8n to use your Runpod endpoint as an OpenAI-compatible API.

<Steps>
  <Step title="Add an OpenAI Chat Model node">
    In your n8n workflow, add a new **OpenAI Chat Model** node to your canvas. Double-click the node to configure it.
  </Step>

  <Step title="Create a new credential">
    Click the dropdown under **Credential to connect with** and select **Create new credential**.
  </Step>

  <Step title="Add your Runpod API key">
    Under **API Key**, add your Runpod API Key. You can create an API key in the [Runpod console](/get-started/api-keys).
  </Step>

  <Step title="Configure the base URL">
    Under **Base URL**, replace the default OpenAI URL with your Runpod endpoint URL:
    
    ```
    https://api.runpod.ai/v2/ENDPOINT_ID/openai/v1
    ```
    
    Replace `ENDPOINT_ID` with your endpoint ID from Step 1.
  </Step>

  <Step title="Save the credential">
    Click **Save**. n8n will automatically test your endpoint connection. If successful, you can start using the node in your workflow.
  </Step>
</Steps>

## Step 3: Test your integration

Create a simple workflow to test your integration.

{/* TODO ... */}

<Steps>
  <Step title="Create a test workflow">
    Add a **Manual Trigger** node and connect it to your **OpenAI Chat Model** node.
  </Step>

  <Step title="Configure the chat model">
    In the **OpenAI Chat Model** node, add a test message like "Hello, what can you help me with?"
  </Step>

  <Step title="Execute the workflow">
    Click **Execute Workflow** in n8n. You should see a response from your model running on Runpod.
  </Step>

  <Step title="Monitor requests">
    Monitor requests from your n8n workflow in the endpoint details page of the Runpod console.
  </Step>
</Steps>

<Note>

The n8n chat feature may have trouble parsing output from vLLM depending on your model. If you experience issues, try adjusting your model's output format or testing with a different model.

</Note>

## Next steps

Now that you've integrated Runpod with n8n, you can:

* Build complex AI-powered workflows using your Runpod endpoints.
* Explore other [integration options](/integrations/overview) with Runpod.
* Learn about [OpenAI compatibility](/serverless/vllm/openai-compatibility) features in vLLM.
