---
title: "Integrate Runpod with n8n"
description: "Learn how to deploy a vLLM worker on Runpod and connect it to n8n for AI-powered workflow automation."
---

Learn how to integrate Runpod Serverless with n8n, a workflow automation tool. By the end of this tutorial, you'll have a vLLM endpoint running on Runpod that you can use within your n8n workflows.

## What you'll learn

In this tutorial, you'll learn how to:

* Deploy a vLLM worker on Runpod Serverless.
* Configure your vLLM endpoint for OpenAI compatibility.
* Connect n8n to your Runpod endpoint.
* Test your integration with a simple workflow.

## Requirements

* You've [created a Runpod account](/get-started/manage-accounts).
* You've created a [Runpod API key](/get-started/api-keys).
* You have an n8n instance (cloud or self-hosted).
* (Optional) For gated models, you've created a [Hugging Face access token](https://huggingface.co/docs/hub/en/security-tokens).

## Step 1: Deploy a vLLM worker on Runpod

First, you'll deploy a vLLM worker that will serve your language model.

<Steps>
  <Step title="Navigate to Serverless">
    Open the [Runpod console](https://www.console.runpod.io/serverless) and navigate to the Serverless page.
  </Step>

  <Step title="Select vLLM template">
    Under **Quick Deploy**, find **Serverless vLLM** and click **Configure**.
  </Step>

  <Step title="Configure your model">
    In the deployment modal:
    
    * Select a vLLM version (latest stable recommended).
    * Under **Hugging Face Models**, enter your model name (e.g., `openchat/openchat-3.5-0106`).
    * If using a gated model like Llama, enter your **Hugging Face Token**.
    * Click **Next**.
  </Step>

  <Step title="Configure vLLM settings">
    In the vLLM settings modal, under **LLM Settings**:
    
    * Set **Max Model Length** to `8192` (or an appropriate context length for your model).
    * If your n8n workflows require tool calling, add environment variables like `ENABLE_AUTO_TOOL_CHOICE=true` and appropriate parsers. See the [vLLM tool calling documentation](https://docs.vllm.ai/en/latest/features/tool_calling.html) for details.
    * Click **Next**.
  </Step>

  <Step title="Configure endpoint settings">
    Review the endpoint settings:
    
    * Set your desired GPU type and worker count.
    * Configure scaling settings based on your expected workload.
    * Click **Deploy**.
  </Step>

  <Step title="Wait for deployment">
    Your endpoint will now begin initializing. This may take several minutes while Runpod provisions resources and downloads your model. Wait until the status shows as **Running**.
  </Step>

  <Step title="Copy your endpoint ID">
    Once deployed, navigate to your endpoint in the Runpod console and copy the **Endpoint ID**. You'll need this to connect n8n.
  </Step>
</Steps>

<Tip>

For more details on vLLM deployment options, see [Deploy a vLLM worker](/serverless/vllm/get-started).

</Tip>

## Step 2: Connect n8n to your Runpod endpoint

Now you'll configure n8n to use your Runpod endpoint as an OpenAI-compatible API.

<Steps>
  <Step title="Add an OpenAI Chat Model node">
    In your n8n workflow, add a new **OpenAI Chat Model** node to your canvas.
  </Step>

  <Step title="Create a new credential">
    Click the dropdown under **Credential to connect with** and select **Create new credential**.
  </Step>

  <Step title="Add your Runpod API key">
    Under **API Key**, add your Runpod API Key. You can find or create API keys in the [Runpod console](https://www.runpod.io/console/user/settings).
  </Step>

  <Step title="Configure the base URL">
    Under **Base URL**, replace the default OpenAI URL with your Runpod endpoint URL:
    
    ```
    https://api.runpod.ai/v2/ENDPOINT_ID/openai/v1
    ```
    
    Replace `ENDPOINT_ID` with your actual endpoint ID from Step 1.
  </Step>

  <Step title="Save the credential">
    Click **Save**. n8n will automatically test your endpoint connection. If successful, you can start using the node in your workflow.
  </Step>
</Steps>

## Step 3: Test your integration

Create a simple workflow to test your integration.

<Steps>
  <Step title="Create a test workflow">
    Add a **Manual Trigger** node and connect it to your **OpenAI Chat Model** node.
  </Step>

  <Step title="Configure the chat model">
    In the **OpenAI Chat Model** node, add a test message like "Hello, what can you help me with?"
  </Step>

  <Step title="Execute the workflow">
    Click **Execute Workflow** in n8n. You should see a response from your model running on Runpod.
  </Step>

  <Step title="Monitor requests">
    Monitor requests from your n8n workflow in the endpoint details page of the Runpod console.
  </Step>
</Steps>

<Note>

The n8n chat feature may have trouble parsing output from vLLM depending on your model. If you experience issues, try adjusting your model's output format or testing with a different model.

</Note>

## Next steps

Now that you've integrated Runpod with n8n, you can:

* Build complex AI-powered workflows using your Runpod endpoints.
* Explore other [integration options](/integrations/overview) with Runpod.
* Learn about [OpenAI compatibility](/serverless/vllm/openai-compatibility) features in vLLM.
