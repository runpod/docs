---
title: "Integrate Runpod with CrewAI"
description: "Learn how to deploy a vLLM worker on Runpod and connect it to CrewAI for orchestrating autonomous AI agents."
---

Learn how to integrate Runpod Serverless with CrewAI, a framework for orchestrating role-playing autonomous AI agents. By the end of this tutorial, you'll have a vLLM endpoint running on Runpod that you can use to power your CrewAI agents.

## What you'll learn

In this tutorial, you'll learn how to:

* Deploy a vLLM worker on Runpod Serverless.
* Configure your vLLM endpoint for OpenAI compatibility.
* Connect CrewAI to your Runpod endpoint.
* Test your integration with a simple agent.

## Requirements

* You've [created a Runpod account](/get-started/manage-accounts).
* You've created a [Runpod API key](/get-started/api-keys).
* You have CrewAI installed in your development environment.
* (Optional) For gated models, you've created a [Hugging Face access token](https://huggingface.co/docs/hub/en/security-tokens).

## Step 1: Deploy a vLLM worker on Runpod

First, you'll deploy a vLLM worker that will serve your language model.

<Steps>
  <Step title="Navigate to Serverless">
    Open the [Runpod console](https://www.console.runpod.io/serverless) and navigate to the Serverless page.
  </Step>

  <Step title="Select vLLM template">
    Under **Quick Deploy**, find **Serverless vLLM** and click **Configure**.
  </Step>

  <Step title="Configure your model">
    In the deployment modal:
    
    * Select a vLLM version (latest stable recommended).
    * Under **Hugging Face Models**, enter your model name (e.g., `openchat/openchat-3.5-0106`).
    * If using a gated model like Llama, enter your **Hugging Face Token**.
    * Click **Next**.
  </Step>

  <Step title="Configure vLLM settings">
    In the vLLM settings modal, under **LLM Settings**:
    
    * Set **Max Model Length** to `8192` (or an appropriate context length for your model).
    * If your CrewAI agents require tool calling, add environment variables like `ENABLE_AUTO_TOOL_CHOICE=true` and appropriate parsers. See the [vLLM tool calling documentation](https://docs.vllm.ai/en/latest/features/tool_calling.html) for details.
    * Click **Next**.
  </Step>

  <Step title="Configure endpoint settings">
    Review the endpoint settings:
    
    * Set your desired GPU type and worker count.
    * Configure scaling settings based on your expected workload.
    * Click **Deploy**.
  </Step>

  <Step title="Wait for deployment">
    Your endpoint will now begin initializing. This may take several minutes while Runpod provisions resources and downloads your model. Wait until the status shows as **Running**.
  </Step>

  <Step title="Copy your endpoint ID">
    Once deployed, navigate to your endpoint in the Runpod console and copy the **Endpoint ID**. You'll need this to connect CrewAI.
  </Step>
</Steps>

<Tip>

For more details on vLLM deployment options, see [Deploy a vLLM worker](/serverless/vllm/get-started).

</Tip>

## Step 2: Connect CrewAI to your Runpod endpoint

Now you'll configure CrewAI to use your Runpod endpoint as an OpenAI-compatible API.

<Steps>
  <Step title="Open LLM connections settings">
    Open the CrewAI dashboard and look for the **LLM connections** section.
  </Step>

  <Step title="Select custom OpenAI provider">
    Under **Provider**, select **custom-openai-compatible** from the dropdown menu.
  </Step>

  <Step title="Add your Runpod API key">
    Configure the connection with your Runpod credentials:
    
    * For `OPENAI_API_KEY`, use your Runpod API Key. You can find or create API keys in the [Runpod console](https://www.runpod.io/console/user/settings).
  </Step>

  <Step title="Configure the base URL">
    For `OPENAI_API_BASE`, add the base URL for your vLLM's OpenAI-compatible endpoint:
    
    ```
    https://api.runpod.ai/v2/ENDPOINT_ID/openai/v1
    ```
    
    Replace `ENDPOINT_ID` with your actual endpoint ID from Step 1.
  </Step>

  <Step title="Test the connection">
    Click **Fetch Available Models** to test the connection. If successful, CrewAI will retrieve the list of models available on your endpoint.
  </Step>
</Steps>

## Step 3: Test your integration

Verify that your CrewAI agents can use your Runpod endpoint.

<Steps>
  <Step title="Create a test agent">
    Create a simple CrewAI agent that uses your Runpod endpoint for its language model.
  </Step>

  <Step title="Run a test task">
    Assign a simple task to your agent and run it to verify that it can communicate with your Runpod endpoint.
  </Step>

  <Step title="Monitor requests">
    Monitor requests from your CrewAI agents in the endpoint details page of the Runpod console.
  </Step>

  <Step title="Verify responses">
    Confirm that your agent is receiving appropriate responses from your model running on Runpod.
  </Step>
</Steps>

## Next steps

Now that you've integrated Runpod with CrewAI, you can:

* Build complex multi-agent systems using your Runpod endpoint to serve the necessary models.
* Explore other [integration options](/integrations/overview).
* Learn more about [OpenAI compatibility](/serverless/vllm/openai-compatibility) features in vLLM.
