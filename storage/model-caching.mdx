---
title: "Pre-cached models"
sidebarTitle: "Pre-cached models"
description: "Use pre-cached models to speed up worker starts and reduce costs for your Serverless endpoints."
tag: "BETA"
---

<Note>
Pre-cached models are currently in beta, and only available for Serverless endpoints. [Join our Discord](https://discord.gg/runpod) if you'd like to provide feedback.
</Note>

This guide provides an overview of how to use cached models with your Serverless endpoints. By using pre-cached AI models, you can significantly reduce worker start times, lower costs, and improve the reliability of your Serverless endpoints.

## Why use pre-cached models?

Using cached models provides several key advantages:

- **Faster cold start times:** Public models or private models stored in the repository are pre-cached on Runpod's infrastructure, eliminating the need for workers to download them from external sources like Hugging Face.
- **Reduced costs:** You aren't billed for worker time while your model is being downloaded. This is especially impactful for large models that can take several minutes to load.
- **Accelerated deployment:** Deploy pre-cached models instantly without waiting for external downloads or transfers.
- **Smaller container images:** By decoupling models from your container image, you can create smaller, more focused images that contain only your serving logic.

## How it works

When you select a model during [Serverless endpoint creation](#use-models-in-serverless), Runpod automatically tries to start your workers on hosts that already contain your selected model.

If no pre-cached host machines are available, the system delays starting your workers until the model download completes, ensuring you still won't be charged for the download time.

## Using pre-cached models in Serverless

Follow these steps to select and add a pre-cached model to your endpoint:

1. Navigate to the [Serverless section](https://www.console.runpod.io/serverless) of the Runpod console.
2. Click **New Endpoint**.
3. In the **Endpoint Configuration** step, scroll down to **Model (optional)** and add the link for the model you want to use (for example, `https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct`).
4. Enter a Hugging Face token if you're using a gated model.
5. Complete your endpoint configuration and click **Deploy Endpoint**.

You can add a pre-cached model to an existing endpoint by selecting **Manage â†’ Edit Endpoint** in the endpoint details page.