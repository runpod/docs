{
  "label": "vLLM workers",
  "position": 9,
  "link": {
    "type": "generated-index",
    "description": "Deploy blazingly fast OpenAI-compatible serverless endpoints for any LLM."
  }
}
