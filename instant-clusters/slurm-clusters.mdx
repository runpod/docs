---
title: Slurm Clusters
sidebarTitle: Slurm Clusters
description: Deploy fully managed Slurm Clusters on Runpod with zero configuration
tag: "BETA"
---

<Note>
Slurm Clusters are currently in beta. If you'd like to provide feedback, please [join our Discord](https://discord.gg/runpod).
</Note>

Runpod Slurm Clusters provide a fully managed high-performance computing and scheduling solution that enables you to create, scale, and manage Slurm Clusters without the complexity of setup and configuration.

Slurm Clusters eliminate the traditional complexity of cluster orchestration by providing:

- **Zero configuration setup:** Slurm is pre-installed and fully configured.
- **Instant provisioning:** Clusters deploy rapidly with minimal setup.
- **Automatic role assignment:** Runpod automatically designates controller and agent nodes.
- **Built-in optimizations:** Pre-configured for optimal NCCL performance.
- **Full Slurm compatibility:** All standard Slurm commands work out-of-the-box.

<Tip>

If you prefer to manually configure your Slurm deployment, see [Deploy an Instant Cluster with Slurm (unmanaged)](/instant-clusters/slurm) for a step-by-step guide.

</Tip>

## Deploy a Slurm Cluster

1. Open the [Instant Clusters page](https://console.runpod.io/cluster) on the Runpod console.
2. Click **Create Cluster**.
3. Select **Slurm Cluster** from the cluster type dropdown menu.
4. Configure your cluster specifications:
   - **Cluster name**: Enter a descriptive name for your cluster.
   - **Pod count**: Choose the number of Pods in your cluster.
   - **GPU type**: Select your preferred [GPU type](/references/gpu-types).
   - **Region**: Choose your deployment region.
   - **Network volume** (optional): Add a [network volume](/pods/storage/create-network-volumes) for persistent/shared storage. If using a network volume, ensure the region matches your cluster region.
   - **Pod template**: Select a [Pod template](/pods/templates/overview) or click **Edit Template** to customize start commands, environment variables, ports, or [container/volume disk](/pods/storage/types) capacity.
5. Click **Deploy Cluster**.

## Access your cluster

Once deployment completes, you can access your cluster from the [Instant Clusters page](https://console.runpod.io/cluster).

Select your cluster to view details. The interface displays the **Slurm Controller** (primary node) and **Slurm Agents** (secondary nodes) with their status and specifications. You can expand each node to view details on each node's resources and status.

You can connect to a node using the **Connect** button, or using any of the [connection methods supported by Pods](/pods/connect-to-pods).

## Submit and manage jobs

All standard Slurm commands are available without configuration:

Check cluster status and available resources:
```bash
sinfo
```

Submit a job to the cluster:
```bash
sbatch your-job-script.sh
```

Monitor job queue and status:
```bash
squeue
```

View detailed job information:
```bash
scontrol show job <job-id>
```

The managed Slurm environment includes:
- Pre-installed Slurm with all necessary plugins.
- Pre-configured Munge authentication.
- Optimized `topology.conf` for NCCL performance.
- Support for common high-performance computing workloads and MPI integration.

## Advanced configuration

While Slurm Clusters work out-of-the-box, you can customize your configuration by connecting to the Slurm Controller node using the [web terminal or SSH](/pods/connect-to-pods).

Access Slurm configuration files in their standard locations:
- `/etc/slurm/slurm.conf` - Main configuration file
- `/etc/slurm/topology.conf` - Network topology configuration
- `/etc/slurm/gres.conf` - Generic resource configuration

Modify these files as needed for your specific requirements. The managed service ensures baseline functionality while allowing flexibility for customization.

## Performance optimization

Managed Slurm clusters come pre-optimized for distributed training workloads:

- **Topology-aware scheduling** - Properly configured topology.conf ensures optimal NCCL performance
- **GPU resource management** - Automatic GRES configuration for GPU scheduling
- **MPI integration** - Pre-configured for seamless MPI job execution

These optimizations enable efficient large-scale ML training without manual tuning.

## Monitor and scale

Monitor your cluster health and resource utilization through the Runpod dashboard. Key metrics include:

- Node availability and status
- Job queue statistics
- Resource utilization trends

Scale your cluster based on workload demands by adding or removing nodes through the Instant Clusters interface. The managed service handles all reconfiguration automatically.

## Best practices

To maximize the effectiveness of your managed Slurm cluster:

- Start with the minimum nodes required and scale as needed
- Use Slurm's built-in accounting features to track resource usage
- Leverage job arrays for similar tasks to improve scheduling efficiency
- Monitor job performance metrics to optimize resource allocation

## Troubleshooting

If you encounter issues with your managed Slurm cluster:

**Jobs stuck in pending state**
Check resource availability with `sinfo` and ensure requested resources are available.

**Authentication errors**
Munge is pre-configured, but if issues arise, verify the munge service is running on all nodes.

**Performance issues**
Review topology configuration and ensure jobs are using appropriate resource requests.

For additional support, contact Runpod support with your cluster ID and specific error messages.