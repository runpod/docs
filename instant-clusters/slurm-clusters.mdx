---
title: Slurm Clusters
sidebarTitle: Slurm Clusters
description: Deploy fully managed Slurm Clusters on Runpod with zero configuration
tag: "BETA"
---

<Note>
Slurm Clusters are currently in beta. If you'd like to provide feedback, please [join our Discord](https://discord.gg/runpod).
</Note>

Runpod Slurm Clusters provide a fully managed high-performance computing and scheduling solution that enables you to create, scale, and manage Slurm Clusters without the complexity of setup and configuration.

Slurm Clusters eliminate the traditional complexity of cluster orchestration by providing:

- **Zero configuration setup:** Slurm is pre-installed and fully configured.
- **Instant provisioning:** Clusters deploy rapidly with minimal setup.
- **Automatic role assignment:** Runpod automatically designates controller and agent nodes.
- **Built-in optimizations:** Pre-configured for optimal NCCL performance.
- **Full Slurm compatibility:** All standard Slurm commands work out-of-the-box.

<Tip>

If you prefer to manually configure your Slurm deployment, see [Deploy an Instant Cluster with Slurm (unmanaged)](/instant-clusters/slurm) for a step-by-step guide.

</Tip>

## Deploy a Slurm Cluster

1. Open the [Instant Clusters page](https://console.runpod.io/cluster) on the Runpod console.
2. Click **Create Cluster**.
3. Select **Slurm Cluster** from the cluster type dropdown menu.
4. Configure your cluster specifications:
   - **Cluster name**: Enter a descriptive name for your cluster.
   - **Pod count**: Choose the number of Pods in your cluster.
   - **GPU type**: Select your preferred [GPU type](/references/gpu-types).
   - **Region**: Choose your deployment region.
   - **Network volume** (optional): Add a [network volume](/pods/storage/create-network-volumes) for persistent/shared storage. If using a network volume, ensure the region matches your cluster region.
   - **Pod template**: Select a [Pod template](/pods/templates/overview) or click **Edit Template** to customize start commands, environment variables, ports, or [container/volume disk](/pods/storage/types) capacity.
5. Click **Deploy Cluster**.

## Access your cluster

Once deployment completes, you can access your cluster from the [Instant Clusters page](https://console.runpod.io/cluster).

Select your cluster to view details. The interface displays the **Slurm Controller** (primary node) and **Slurm Agents** (secondary nodes) with their status and specifications. You can expand each node to view details on each node's resources and status.

You can connect to a node using the **Connect** button, or using any of the [connection methods supported by Pods](/pods/connect-to-pods).

## Submit and manage jobs

All standard Slurm commands are available without configuration:

Check cluster status and available resources:
```bash
sinfo
```

Submit a job to the cluster from the Slurm controller node:
```bash
sbatch your-job-script.sh
```

Monitor job queue and status:
```bash
squeue
```

View detailed job information from the Slurm controller node:
```bash
scontrol show job <job-id>
```

The managed Slurm environment includes:
- Pre-installed Slurm with all necessary plugins.
- Pre-configured Munge authentication.
- Optimized `topology.conf` for NCCL performance.
- Support for common high-performance computing workloads and MPI integration.

## Advanced configuration

While Runpod's Slurm Clusters work out-of-the-box, you can customize your configuration by connecting to the Slurm controller node using the [web terminal or SSH](/pods/connect-to-pods).

Access Slurm configuration files in their standard locations:
- `/etc/slurm/slurm.conf` - Main configuration file.
- `/etc/slurm/topology.conf` - Network topology configuration.
- `/etc/slurm/gres.conf` - Generic resource configuration.

Modify these files as needed for your specific requirements.

## Monitoring

Monitor your cluster health and resource utilization through the Runpod console. The interface provides visibility into metrics like node availability and status, job queue statistics, and resource utilization trends to help you optimize cluster performance.

## Troubleshooting

If you encounter issues with your Slurm Cluster:

**Jobs stuck in pending state:** Check resource availability with `sinfo` and ensure requested resources are available. If you need more resources, you can add more nodes to your cluster.

**Authentication errors:** Munge is pre-configured, but if issues arise, verify the munge service is running on all nodes.

**Performance issues:** Review topology configuration and ensure jobs are using appropriate resource requests.

For additional support, contact Runpod support with your cluster ID and specific error messages.