---
title: Slurm Clusters
sidebarTitle: Slurm Clusters
description: Deploy fully managed Slurm Clusters on Runpod with zero configuration
tag: "NEW"
---

Runpod Slurm Clusters provide a fully managed high-performance computing scheduling solution that enables you to create, scale, and manage Slurm Clusters without the complexity of setup and configuration.

Slurm Clusters eliminate the traditional complexity of cluster orchestration by providing:

- **Zero configuration setup:** Slurm is pre-installed and fully configured.
- **Instant provisioning:** Clusters deploy rapidly with minimal setup.
- **Automatic role assignment:** Runpod automatically designates controller and worker nodes.
- **Built-in optimizations:** Pre-configured for optimal NCCL performance.
- **Full Slurm compatibility:** All standard Slurm commands work out-of-the-box.

This page shows how to deploy a Slurm Cluster on Runpod, and how to use it to run distributed training jobs.

<Tip>

If you prefer to manually configure and manage your Slurm deployment, see [Deploy an Instant Cluster with Slurm (unmanaged)](/instant-clusters/slurm) for a step-by-step guide.

</Tip>

## Deploy a Slurm Cluster

1. Navigate to the [Instant Cluster page](https://console.runpod.io/cluster) on the Runpod console.
2. Click **Create Cluster**.
3. Select **Slurm Cluster** from the dropdown menu of cluster types.
4. Configure your cluster specifications, including:
    - [Network volume](/pods/storage/create-network-volumes), if you need persistent/shared storage.
    - Region (must be the same as the network volume if you're using one).
    - Pod count (i.e., the number of Pods in the cluster).
    - [GPU type](/references/gpu-types).
    - Cluster name.
    - [Pod template](/pods/templates/overview).
        - Click **Edit Template** if you need to adjust start commands, environment variables, ports, or the capacity of the [container/volume disk](/pods/storage/types).
5. Click **Deploy Cluster**.

## Access your cluster

Once deployment completes, you can access your cluster from the [Instant Clusters page](https://console.runpod.io/cluster).

Select your cluster to view details. The interface displays the **Slurm Controller** (primary node) and **Slurm Agents** (secondary nodes) with their status and specifications. You can expand each node to view details on the node's resources and status.

You can connect to each node using the **Connect** button, or with any of the [connection methods supported by Pods](/pods/connect-to-pods).

## Submit and manage jobs

All standard Slurm commands are available without configuration:

Check cluster status and available resources:
```bash
sinfo
```

Submit a job to the cluster:
```bash
sbatch your-job-script.sh
```

Monitor job queue and status:
```bash
squeue
```

View detailed job information:
```bash
scontrol show job <job-id>
```

The managed Slurm environment includes:
- Pre-installed Slurm with all necessary plugins.
- Pre-configured Munge authentication.
- Optimized `topology.conf` for NCCL performance.
- Support for common high-performance computing workloads and MPI integration.

## Advanced configuration

While the managed Slurm cluster works out-of-the-box, advanced users can customize configurations through shell access to the Slurm Controller node.

Access Slurm configuration files in their standard locations:
- `/etc/slurm/slurm.conf` - Main configuration file
- `/etc/slurm/topology.conf` - Network topology configuration
- `/etc/slurm/gres.conf` - Generic resource configuration

Modify these files as needed for your specific requirements. The managed service ensures baseline functionality while allowing flexibility for customization.

## Performance optimization

Managed Slurm clusters come pre-optimized for distributed training workloads:

- **Topology-aware scheduling** - Properly configured topology.conf ensures optimal NCCL performance
- **GPU resource management** - Automatic GRES configuration for GPU scheduling
- **MPI integration** - Pre-configured for seamless MPI job execution

These optimizations enable efficient large-scale ML training without manual tuning.

## Monitor and scale

Monitor your cluster health and resource utilization through the Runpod dashboard. Key metrics include:

- Node availability and status
- Job queue statistics
- Resource utilization trends

Scale your cluster based on workload demands by adding or removing nodes through the Instant Clusters interface. The managed service handles all reconfiguration automatically.

## Best practices

To maximize the effectiveness of your managed Slurm cluster:

- Start with the minimum nodes required and scale as needed
- Use Slurm's built-in accounting features to track resource usage
- Leverage job arrays for similar tasks to improve scheduling efficiency
- Monitor job performance metrics to optimize resource allocation

## Troubleshooting

If you encounter issues with your managed Slurm cluster:

**Jobs stuck in pending state**
Check resource availability with `sinfo` and ensure requested resources are available.

**Authentication errors**
Munge is pre-configured, but if issues arise, verify the munge service is running on all nodes.

**Performance issues**
Review topology configuration and ensure jobs are using appropriate resource requests.

For additional support, contact Runpod support with your cluster ID and specific error messages.