---
title: "Integrate Runpod with external tools"
sidebarTitle: "Overview"
description: "Learn how to integrate Runpod endpoints with external tools and frameworks."
---

Runpod can be integrated with any system that supports custom endpoint configuration. If a library or framework lets you set a custom base URL, you can connect it to your Runpod endpoints without specialized adapters or connectors.

## How it works

Integration is straightforward: any library or framework that accepts a custom base URL for API calls will work with Runpod. This means you can use Runpod with tools like n8n, CrewAI, LangChain, and many others by simply pointing them to your Runpod endpoint URL.

## Deployment options

Runpod offers four deployment options for endpoint integrations:

### Public Endpoints

Public Endpoints are pre-deployed AI models that you can use without setting up your own Serverless endpoint. They're vLLM-compatible and return OpenAI-compatible responses, so you can get started quickly or test things out without deploying infrastructure.

The following Public Endpoint URLs are available for OpenAI-compatible models:

```
# Public Endpoint for Qwen3 32B AWQ
https://api.runpod.ai/v2/qwen3-32b-awq/openai/v1

# Public Endpoint for ibm/IBM Granite-4.0-H-Small
https://api.runpod.ai/v2/granite-4-0-h-small/openai/v1
```

For pricing details and usage information, see [Public Endpoints](/hub/public-endpoints).

### vLLM

[vLLM workers](/serverless/vllm/overview) provide an inference engine that returns [OpenAI-compatible responses](/serverless/vllm/openai-compatibility), making it ideal for tools that expect OpenAI's API format.

When you deploy a vLLM endpoint, access it using the OpenAI-compatible API at:

```
https://api.runpod.ai/v2/ENDPOINT_ID/openai/v1
```

Where `ENDPOINT_ID` is your Serverless endpoint ID.

### SGLang

[SGLang workers](/serverless/sglang/overview) are an inference engine that returns OpenAI-compatible responses. It offers optimized performance for certain model types and use cases.

To learn more, see the [runpod-workers/worker-sglang](https://github.com/runpod-workers/worker-sglang) repository on GitHub.

### Load balancer

[Load balancing endpoints](/serverless/load-balancing/overview) let you create custom endpoints where you define your own inputs and outputs. This gives you complete control over the API contract and is ideal when you need custom behavior beyond standard inference patterns.

## Model configuration for compatibility

Some models require specific vLLM environment variables to work with external tools and frameworks. You may need to set a custom chat template or [tool call parser](https://docs.vllm.ai/en/latest/features/tool_calling.html) to ensure your model returns responses in the format your integration expects.

For example, you can configure the `Qwen/qwen3-32b-awq` model for OpenAI compatibility by adding these environment variables in your vLLM endpoint settings:

```txt
ENABLE_AUTO_TOOL_CHOICE=true
REASONING_PARSER=qwen3
TOOL_CALL_PARSER=hermes
```

These settings enable automatic tool choice selection and set the right parsers for the Qwen3 model to work with tools that expect OpenAI-formatted responses.

For more information about tool calling configuration and available parsers, see the [vLLM tool calling documentation](https://docs.vllm.ai/en/latest/features/tool_calling.html).

## Integration tutorials

Follow these step-by-step tutorials to integrate Runpod with popular tools:

<CardGroup cols={2}>
  <Card title="Integrate with n8n" icon="workflow" href="/tutorials/integrations/n8n-integration">
    Connect Runpod to n8n for AI-powered workflow automation.
  </Card>

  <Card title="Integrate with CrewAI" icon="robot" href="/tutorials/integrations/crewai-integration">
    Use Runpod to power autonomous AI agents in CrewAI.
  </Card>
</CardGroup>

## Compatible frameworks

The same integration pattern works with any framework that supports custom OpenAI-compatible endpoints, including:

* **CrewAI**: A framework for orchestrating role-playing autonomous AI agents.
* **LangChain**: A framework for developing applications powered by language models.
* **AutoGen**: Microsoft's framework for building multi-agent conversational systems.
* **Haystack**: An end-to-end framework for building search systems and question answering.
* **n8n**: A workflow automation tool with AI integration capabilities.

Configure these frameworks to use your Runpod endpoint URL as the base URL, and provide your Runpod API key for authentication.

## Third-party integrations

For infrastructure management and orchestration, Runpod integrates with:

* [**dstack**](/integrations/dstack): Simplified Pod orchestration for AI/ML workloads.
* [**SkyPilot**](/integrations/skypilot): Multi-cloud execution framework.
* [**Mods**](/integrations/mods): AI-powered command-line tool.
