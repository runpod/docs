---
title: "Running Runpod on Mods"
sidebarTitle: "Mods"
---

[Mods](https://github.com/charmbracelet/mods) is a command-line tool for interacting with language models. It integrates with Unix pipelines, letting you send command output directly to LLMs from your terminal.

## How Mods works

Mods reads standard input (or a prompt you provide as an argument), sends it to a language model, and prints the result. You can prefix the input with a prompt from the Mods arguments, and optionally format the output as Markdown. This lets you pipe command output to an LLM for analysis or transformation.

## Get started

<Steps>
  <Step title="Obtain your API key">
    Get your API key from the [Runpod Settings](https://www.console.runpod.io/user/settings) page.
  </Step>

  <Step title="Install Mods">
    Follow the installation instructions for [Mods](https://github.com/charmbracelet/mods) based on your system.
  </Step>

  <Step title="Configure Runpod">
   Update the `config_template.yml` file with your Runpod configuration:

     ```yml
     runpod:
       # https://docs.runpod.io/serverless/vllm/openai-compatibility
       base-url: https://api.runpod.ai/v2/${YOUR_ENDPOINT}/openai/v1
       api-key:
       api-key-env: RUNPOD_API_KEY
       models:
         # Add your model name
         openchat/openchat-3.5-1210:
           aliases: ["openchat"]
           max-input-chars: 8192
     ```

   Replace the following values:
   * `base-url`: Your specific endpoint URL.
   * `api-key-env`: Your Runpod API key.
   * `openchat/openchat-3.5-1210`: The model name you want to use.
   * `aliases: ["openchat"]`: Your preferred model alias.
   * `max-input-chars`: The maximum input characters for your model.
  </Step>

  <Step title="Verify your setup">
    Test your setup by piping command output to Mods:

     ```sh
     ls ~/Downloads | mods --api runpod --model openchat -f "tell my fortune based on these files" | glow
     ```

   This lists files in your `~/Downloads` directory, sends them to Mods using the Runpod API and specified model, and pipes the output to `glow` for formatted display.
  </Step>
</Steps>
