---
title: "Public endpoints"
sidebarTitle: "Public endpoints"
description: "Deploy and use production-ready AI models with RunPod's managed Serverless endpoints."
---

RunPod public endpoints provide instant access to state-of-the-art AI models through simple API calls, eliminating the need to manage infrastructure while offering enterprise-grade scaling and performance. These endpoints are optimized for production workloads and can automatically scale from 1 to 1000+ requests per hour.

<Note>
Public endpoints are currently in beta. We're actively expanding our model selection and improving the user experience. [Join our Discord](https://discord.gg/runpod) for updates and feedback.
</Note>

## Getting started

Public endpoints are available through the RunPod Hub, providing a streamlined way to discover and deploy AI models. Navigate to the [RunPod Hub](https://www.runpod.io/console/hub) in your console and find the **Public endpoints** section. Browse available models and select one that fits your needs, then click **Deploy** to create your endpoint. Once deployed, you can start making API calls immediately.

Our initial launch includes optimized text-to-image generation models. FLUX.1 [dev] offers high-quality image generation with excellent prompt adherence, making it ideal for professional artwork and detailed illustrations. FLUX.1 [schnell] provides fast image generation optimized for speed, perfect for rapid prototyping and real-time applications. More models are coming soon, including video generation capabilities and custom model deployment options.

## Using public endpoints

All requests to public endpoints require authentication using your RunPod API key. You can find your API key in the [RunPod console](https://www.runpod.io/console/user/settings) under Settings > API Keys.

For immediate results, use the synchronous endpoint with `/runsync`:

```bash
curl -X POST "https://api.runpod.ai/v2/{endpoint-id}/runsync" \
  -H "Authorization: Bearer YOUR_RUNPOD_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "input": {
      "prompt": "A serene mountain landscape at sunset",
      "width": 1024,
      "height": 1024,
      "num_inference_steps": 20,
      "guidance_scale": 7.5
    }
  }'
```

For longer-running tasks or batch processing, use the asynchronous endpoint with `/run`:

```bash
curl -X POST "https://api.runpod.ai/v2/{endpoint-id}/run" \
  -H "Authorization: Bearer YOUR_RUNPOD_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "input": {
      "prompt": "A futuristic cityscape with flying cars",
      "width": 1024,
      "height": 1024,
      "num_inference_steps": 50,
      "guidance_scale": 8.0
    }
  }'
```

Then check the status and retrieve results:

```bash
curl -X GET "https://api.runpod.ai/v2/{endpoint-id}/status/{job-id}" \
  -H "Authorization: Bearer YOUR_RUNPOD_API_KEY"
```

All endpoints return a consistent JSON response format:

```json
{
  "id": "job-12345",
  "status": "COMPLETED",
  "output": {
    "images": [
      {
        "url": "https://runpod.s3.amazonaws.com/...",
        "base64": "data:image/png;base64,..."
      }
    ]
  },
  "executionTime": 2.5,
  "cost": 0.0075
}
```

## Model parameters

### FLUX.1 [dev]

FLUX.1 [dev] is optimized for high-quality, detailed image generation. The model accepts several parameters to control the generation process:

```json
{
  "input": {
    "prompt": "Your image description",
    "negative_prompt": "Elements to avoid (optional)",
    "width": 1024,
    "height": 1024,
    "num_inference_steps": 20,
    "guidance_scale": 7.5,
    "seed": 42,
    "output_format": "png"
  }
}
```

The `prompt` parameter is required and should contain a text description of the desired image. The `negative_prompt` parameter is optional and can be used to specify elements to exclude from the image. Image dimensions can be controlled with `width` and `height` parameters, both accepting values from 64 to 2048 pixels with a default of 1024. The `num_inference_steps` parameter controls the number of denoising steps, accepting values from 1 to 50 with a default of 20. Higher values generally produce better quality but take longer to generate.

The `guidance_scale` parameter determines how closely the model follows the prompt, accepting values from 1.0 to 20.0 with a default of 7.5. Higher values make the model follow the prompt more strictly. You can set a `seed` parameter for reproducible results, and choose the `output_format` as either "png" or "jpeg".

### FLUX.1 [schnell]

FLUX.1 [schnell] is optimized for speed and real-time applications:

```json
{
  "input": {
    "prompt": "A quick sketch of a mountain",
    "width": 1024,
    "height": 1024,
    "num_inference_steps": 4,
    "guidance_scale": 1.0,
    "seed": 123
  }
}
```

Similar to FLUX.1 [dev], the `prompt` parameter is required for describing the desired image. The `width` and `height` parameters accept values from 64 to 2048 pixels. However, FLUX.1 [schnell] is optimized for fewer inference steps, with `num_inference_steps` accepting values from 1 to 8 and defaulting to 4. The `guidance_scale` parameter has a narrower range of 0.5 to 2.0 with a default of 1.0, as this model works best with lower guidance values.

<Warning>
FLUX.1 [schnell] is optimized for speed and works best with lower step counts. Using higher values may not improve quality significantly.
</Warning>

## Code examples

### Python

Here's a complete Python example for generating images using public endpoints:

```python
import requests
import json

def generate_image(prompt, endpoint_id, api_key):
    url = f"https://api.runpod.ai/v2/{endpoint_id}/runsync"
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "input": {
            "prompt": prompt,
            "width": 1024,
            "height": 1024,
            "num_inference_steps": 20,
            "guidance_scale": 7.5
        }
    }
    
    response = requests.post(url, headers=headers, json=payload)
    
    if response.status_code == 200:
        result = response.json()
        return result["output"]["images"][0]["url"]
    else:
        raise Exception(f"Error: {response.status_code} - {response.text}")

# Usage
image_url = generate_image(
    "A beautiful sunset over the ocean",
    "your-endpoint-id",
    "your-api-key"
)
print(f"Generated image: {image_url}")
```

### Go

This Go example demonstrates how to integrate public endpoints into your Go applications:

```go
package main

import (
    "bytes"
    "encoding/json"
    "fmt"
    "io"
    "net/http"
)

type Request struct {
    Input Input `json:"input"`
}

type Input struct {
    Prompt            string  `json:"prompt"`
    Width             int     `json:"width"`
    Height            int     `json:"height"`
    NumInferenceSteps int     `json:"num_inference_steps"`
    GuidanceScale     float64 `json:"guidance_scale"`
}

type Response struct {
    Output struct {
        Images []struct {
            URL string `json:"url"`
        } `json:"images"`
    } `json:"output"`
}

func generateImage(prompt, endpointID, apiKey string) (string, error) {
    url := fmt.Sprintf("https://api.runpod.ai/v2/%s/runsync", endpointID)
    
    req := Request{
        Input: Input{
            Prompt:            prompt,
            Width:             1024,
            Height:            1024,
            NumInferenceSteps: 20,
            GuidanceScale:     7.5,
        },
    }
    
    jsonData, err := json.Marshal(req)
    if err != nil {
        return "", err
    }
    
    httpReq, err := http.NewRequest("POST", url, bytes.NewBuffer(jsonData))
    if err != nil {
        return "", err
    }
    
    httpReq.Header.Set("Authorization", "Bearer "+apiKey)
    httpReq.Header.Set("Content-Type", "application/json")
    
    client := &http.Client{}
    resp, err := client.Do(httpReq)
    if err != nil {
        return "", err
    }
    defer resp.Body.Close()
    
    body, err := io.ReadAll(resp.Body)
    if err != nil {
        return "", err
    }
    
    var response Response
    err = json.Unmarshal(body, &response)
    if err != nil {
        return "", err
    }
    
    return response.Output.Images[0].URL, nil
}

func main() {
    imageURL, err := generateImage(
        "A majestic eagle soaring through clouds",
        "your-endpoint-id",
        "your-api-key",
    )
    if err != nil {
        fmt.Println("Error:", err)
        return
    }
    
    fmt.Println("Generated image:", imageURL)
}
```

### Rust

For Rust applications, here's a complete example using the reqwest and serde crates:

```rust
use reqwest;
use serde::{Deserialize, Serialize};

#[derive(Serialize)]
struct GenerateRequest {
    input: GenerateInput,
}

#[derive(Serialize)]
struct GenerateInput {
    prompt: String,
    width: u32,
    height: u32,
    num_inference_steps: u32,
    guidance_scale: f32,
}

#[derive(Deserialize)]
struct GenerateResponse {
    output: GenerateOutput,
}

#[derive(Deserialize)]
struct GenerateOutput {
    images: Vec<GeneratedImage>,
}

#[derive(Deserialize)]
struct GeneratedImage {
    url: String,
}

async fn generate_image(
    prompt: &str,
    endpoint_id: &str,
    api_key: &str,
) -> Result<String, Box<dyn std::error::Error>> {
    let client = reqwest::Client::new();
    let url = format!("https://api.runpod.ai/v2/{}/runsync", endpoint_id);
    
    let request_body = GenerateRequest {
        input: GenerateInput {
            prompt: prompt.to_string(),
            width: 1024,
            height: 1024,
            num_inference_steps: 20,
            guidance_scale: 7.5,
        },
    };
    
    let response = client
        .post(&url)
        .header("Authorization", format!("Bearer {}", api_key))
        .header("Content-Type", "application/json")
        .json(&request_body)
        .send()
        .await?;
    
    let result: GenerateResponse = response.json().await?;
    Ok(result.output.images[0].url.clone())
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let image_url = generate_image(
        "A cyberpunk cityscape at night with neon lights",
        "your-endpoint-id",
        "your-api-key",
    ).await?;
    
    println!("Generated image: {}", image_url);
    Ok(())
}
```

## Using the playground

The RunPod Hub includes an integrated playground for testing and experimenting with public endpoints. To access the playground, navigate to your deployed public endpoint in the RunPod console and click **Open playground** in the endpoint details. The playground opens with your endpoint pre-configured and ready for testing.

The playground provides interactive parameter adjustment, allowing you to modify prompts, dimensions, and model settings in real-time. You can generate images directly in the browser with instant preview capabilities. The playground also shows cost estimation, letting you see estimated costs before running generation. It generates API code examples that you can copy for your applications and provides result sharing functionality to share generated images and configurations with your team.

The playground is ideal for prompt engineering, where you can iterate on prompts to achieve desired results. It's also perfect for parameter optimization to find the best settings for your use case, quality assessment to compare different models and configurations, and integration planning to test API responses before building applications.

## Pricing

Public endpoints use transparent, usage-based pricing based on megapixels generated. FLUX.1 [dev] costs $0.025 per megapixel, while FLUX.1 [schnell] costs $0.003 per megapixel.

For a 1024×1024 image (1 megapixel), you'll pay $0.025 for FLUX.1 [dev] or $0.003 for FLUX.1 [schnell]. A 512×512 image (0.25 megapixels) costs $0.00625 for dev or $0.00075 for schnell. A larger 2048×2048 image (4 megapixels) costs $0.10 for dev or $0.012 for schnell.

<Note>
Pricing is calculated based on the actual output resolution. Failed generations are not charged.
</Note>

## Best practices

## Migration from custom endpoints

If you're currently using custom Serverless endpoints for image generation, public endpoints offer several advantages. You'll benefit from reduced maintenance since there's no need to manage Docker images or worker code. The models are pre-optimized for RunPod's infrastructure, providing better performance. Built-in scaling handles from 1 to 1000+ requests per hour automatically. You'll also enjoy cost efficiency by paying only for successful generations, and regular updates ensure models are automatically updated with improvements.

To migrate from custom endpoints, start by identifying equivalent models and mapping your current models to available public endpoints. Update your API calls to use the new endpoint format, and test thoroughly using the playground to validate that behavior matches your expectations. Make sure to update authentication to use the correct API key format, and monitor usage to track costs and performance after migration.

## Support and community


## Migration from custom endpoints

If you're currently using custom serverless endpoints for image generation, public endpoints offer several advantages:

### Benefits of migration

- **Reduced maintenance**: No need to manage Docker images or worker code
- **Optimized performance**: Models are pre-optimized for RunPod's infrastructure
- **Automatic scaling**: Built-in scaling from 1 to 1000+ requests per hour
- **Cost efficiency**: Pay only for successful generations
- **Regular updates**: Models are automatically updated with improvements

### Migration steps

1. **Identify equivalent models**: Map your current models to available public endpoints
2. **Update API calls**: Modify your code to use the new endpoint format
3. **Test thoroughly**: Use the playground to validate behavior matches expectations
4. **Update authentication**: Ensure you're using the correct API key format
5. **Monitor usage**: Track costs and performance after migration
