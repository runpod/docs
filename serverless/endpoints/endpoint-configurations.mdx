---
title: "Endpoint settings and optimization guide"
sidebarTitle: "Endpoint settings"
description: "Configure your endpoints to optimize for performance, cost, and reliability."
---

import GPUTable from '/snippets/serverless-gpu-pricing-table.mdx';

This guide details the configuration options available for Runpod Serverless endpoints. These settings control how your endpoint scales, how it utilizes hardware, and how it manages request lifecycles.

<Frame alt="Endpoint configurations">
<img src="/images/endpoint-settings.png" />
</Frame>

## General configuration

### Endpoint name

The name assigned to your endpoint helps you identify it within the Runpod console. This is a local display name and does not impact the endpoint ID used for API requests.

### Endpoint type

Select the architecture that best fits your application's traffic pattern:

**Queue based endpoints** utilize a built-in queueing system to manage requests. They are ideal for asynchronous tasks, batch processing, and long-running jobs where immediate synchronous responses are not required. These endpoints provide guaranteed execution and automatic retries for failed requests.

Queue based endpoints are implemented using [handler functions](/serverless/workers/handler-functions).

**Load balancing endpoints** route traffic directly to available workers, bypassing the internal queue. They are designed for high-throughput, low-latency applications that require synchronous request/response cycles, such as real-time inference or custom REST APIs. 

For implementation details, see [Load balancing endpoints](https://www.google.com/search?q=/serverless/load-balancing/overview).

### GPU configuration

This setting determines the hardware tier your workers will utilize. You can select multiple GPU categories to create a prioritized list. Runpod attempts to allocate the first category in your list. If that hardware is unavailable, it automatically falls back to the subsequent options. 

Selecting multiple GPU types significantly improves endpoint availability during periods of high demand.

<GPUTable />

## Worker scaling

### Active workers

This setting defines the minimum number of workers that remain warm and ready to process requests at all times. Setting this to 1 or higher eliminates cold starts for the initial wave of requests. Active workers incur charges even when idle, but they receive a 20-30% discount compared to on-demand workers.

<Tip>

For workloads with long cold start times, use active workers to eliminate startup delays. You can estimate the optimal number by:

1. Measuring your requests per minute during typical usage.
2. Calculating average request duration in seconds.
3. Using the formula: Active Workers = (Requests per Minute × Request Duration) / 60

For example, with 6 requests per minute taking 30 seconds each: 6 × 30 / 60 = 3 active workers.

Even a small number of active workers can significantly improve performance for steady traffic patterns while maintaining cost efficiency.

</Tip>

### Max workers

This setting controls the maximum number of concurrent instances your endpoint can scale to. This acts as a safety limit for costs and a cap on concurrency.

<Tip> 
We recommend setting your max worker count approximately 20% higher than your expected maximum concurrency. This buffer allows for smoother scaling during traffic spikes. 

Avoid setting this to 1, as this restricts your deployment to a single machine, creating potential bottlenecks if that machine becomes unavailable.
</Tip>

### GPUs per worker

This defines how many GPUs are assigned to a single worker instance. The default is 1.

When choosing between multiple lower-tier GPUs or fewer high-end GPUs, you should generally prioritize high-end GPUs with lower GPU count per worker when possible.

- High-end GPUs typically offer faster memory speeds and newer architectures, improving model loading and inference times.
- Multi-GPU configurations introduce parallel processing overhead that can offset performance gains.
- Higher GPU-per-worker requirements can reduce availability, as finding machines with multiple free GPUs is more challenging than locating single available GPUs.

### Auto-scaling type

This setting determines the logic used to scale workers up and down.

**Queue delay** scaling adds workers based on wait times. If requests sit in the queue for longer than a defined threshold (default 4 seconds), the system provisions new workers. This is best for workloads where slight delays are acceptable in exchange for higher utilization.

**Request count** scaling is more aggressive. It adjusts worker numbers based on the total volume of pending and active work. The formula used is `Math.ceil((requestsInQueue + requestsInProgress) / scalerValue)`. Use a scaler value of 1 for maximum responsiveness, or increase it to scale more conservatively. This strategy is recommended for LLM workloads or applications with frequent, short requests.

## Lifecycle and timeouts

### Idle timeout

The idle timeout determines how long a worker remains active after completing a request before shutting down. While a worker is idle, you are billed for the time, but the worker remains "warm," allowing it to process subsequent requests immediately. The default is 5 seconds.

<Tip>
If you observe frequent cold starts, consider increasing this value to match your average traffic gaps. However, be aware that if you use the [Queue delay](#auto-scaling-type) scaling strategy, setting this value too high may prevent workers from scaling down properly.
</Tip>

### Execution timeout

The execution timeout acts as a failsafe to prevent runaway jobs from consuming infinite resources. It specifies the maximum duration a single job is allowed to run before being forcibly terminated.

We strongly recommend keeping this enabled. Set the value to your longest expected request duration plus a 20% buffer. The default is 600 seconds (10 minutes), and it can be extended up to 24 hours.

### Job TTL (time-to-live)

This setting defines how long a job request remains valid in the queue before expiring. If a worker does not pick up the job within this window, the system discards it. The default is 24 hours.

## Performance features

### FlashBoot

FlashBoot reduces cold start times by retaining the state of worker resources shortly after they spin down. This allows the system to "revive" a worker much faster than a standard fresh boot. FlashBoot is most effective on endpoints with consistent traffic, where workers frequently cycle between active and idle states. There is no additional cost for enabling FlashBoot.

### Model (optional)

The Model field allows you to select from a list of [cached models](/serverless/endpoints/model-caching). When selected, Runpod schedules your workers on host machines that already have these large model files pre-loaded. This significantly reduces the network time required to download models during initialization.

## Advanced settings

### Data centers

You can restrict your endpoint to specific geographical regions. For maximum reliability and availability, we recommend allowing all data centers. Restricting this list decreases the pool of available GPUs your endpoint can draw from.

### Network volumes

[Network volumes](/storage/network-volumes) provide persistent storage that survives worker restarts. While they enable data sharing between workers, they introduce network latency and restrict your endpoint to the specific data center where the volume resides. Use network volumes only if your workload specifically requires shared persistence or datasets larger than the container limit.

### CUDA version selection

This filter ensures your workers are scheduled on host machines with compatible drivers. While you should select the version your code requires, we recommend also selecting all newer versions. CUDA is generally backward compatible, and selecting a wider range of versions increases the pool of available hardware.

### Expose HTTP/TCP ports

Enabling this option exposes the public IP and port of the worker, allowing for direct external communication. This is required for applications that need persistent connections, such as WebSockets.

## Reducing worker startup times

There are two primary factors that impact worker start times:

1. **Worker initialization time:** Worker initialization occurs when a Docker image is downloaded to a new worker. This takes place after you create a new endpoint, adjust worker counts, or deploy a new worker image. Requests that arrive during initialization face delays, as a worker must be fully initialized before it can start processing.

2. **Cold start:** A cold start occurs when a worker is revived from an idle state. Cold starts can get very long if your handler code loads large ML models (several gigabytes to hundreds of gigabytes) into GPU memory.

<Note>

If your worker's cold start time exceeds the default 7-minute limit (which can occur when loading large models), the system may mark it as unhealthy. To prevent this, you can extend the cold start timeout by setting the `RUNPOD_INIT_TIMEOUT` environment variable. For example, setting `RUNPOD_INIT_TIMEOUT=800` allows up to 800 seconds (13.3 minutes) for revival.

</Note>

Use these strategies to reduce worker startup times:

1. **Embed models in Docker images:** Package your ML models directly within your worker container image instead of downloading them in your handler function. This strategy places models on the worker's high-speed local storage (SSD/NVMe), dramatically reducing the time needed to load models into GPU memory. This approach is optimal for production environments, though extremely large models (500GB+) may require network volume storage.

2. **Store large models on network volumes:** For flexibility during development, save large models to a [network volume](/storage/network-volumes) using a Pod or one-time handler, then mount this volume to your Serverless workers. While network volumes offer slower model loading compared to embedding models directly, they can speed up your workflow by enabling rapid iteration and seamless switching between different models and configurations.

3. **Maintain active workers:** Set active worker counts above zero to completely eliminate cold starts. These workers remain ready to process requests instantly and cost up to 30% less when idle compared to standard (flex) workers.

4. **Extend idle timeouts:** Configure longer idle periods to preserve worker availability between requests. This strategy prevents premature worker shutdown during temporary traffic lulls, ensuring no cold starts for subsequent requests.

5. **Optimize scaling parameters:** Fine-tune your auto-scaling configuration for more responsive worker provisioning:
   - Lower queue delay thresholds to 2-3 seconds (default 4).
   - Decrease request count thresholds to 2-3 (default 4).

   These refinements create a more agile scaling system that responds swiftly to traffic fluctuations.

6. **Increase maximum worker limits:** Set higher maximum worker capacities to ensure your Docker images are pre-cached across multiple compute nodes and data centers. This proactive approach eliminates image download delays during scaling events, significantly reducing startup times.

## Best practices summary

- **Understand optimization tradeoffs** and make conscious tradeoffs between cost, speed, and model size.
- **Start conservative** with [max workers](#max-workers) and scale up as needed.
- **Monitor throttling** and adjust [max workers](#max-workers) accordingly.
- **Use [active workers](#active-workers)** for latency-sensitive applications.
- **Select multiple [GPU types](#gpu-configuration)** to improve availability.
- **Choose appropriate [timeouts](#execution-timeout)** based on your workload characteristics.
- **Consider data locality** when using network volumes.
- **Avoid setting [max workers](#max-workers) to 1** to prevent bottlenecks.
- **Plan for 20% headroom** in [max workers](#max-workers) to handle load spikes.
- **Prefer high-end GPUs with lower GPU count** for better performance.
- **Set [execution timeout](#execution-timeout)** to prevent runaway processes.
- **Match [auto-scaling strategy](#auto-scaling-type)** to your workload patterns.
- **Embed models in Docker images** when possible for faster loading.
- **Extend [idle timeouts](#idle-timeout)** to prevent frequent cold starts.
- **Consider disabling [FlashBoot](#flashboot)** for endpoints with few workers or infrequent traffic.
