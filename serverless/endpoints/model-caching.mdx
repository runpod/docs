---
title: "Cached models"
sidebarTitle: "Cached models"
description: "Accelerate cold starts and reduce costs by using cached models."
tag: "BETA"
---

<Note>
Cached models are currently in beta. [Join our Discord](https://discord.gg/runpod) if you'd like to provide feedback.
</Note>

Enabling cached models on your Serverless endpoint can reduce [cold start times](/serverless/overview#cold-starts) to just a few seconds, and dramatically reduce the cost for loading large models.

## Why use cached models?

- **Faster cold starts:** A "cold start" refers to the delay between when a request is received by an endpoint with no running workers and when a worker is fully "warmed up" and ready to handle the request. This generally involves starting the container, loading models into GPU memory, and initializing runtime environments. Larger models take longer to load into memory, increasing cold start times (and request response times). 
- **Reduced costs:** You aren't billed for worker time while your model is being downloaded. This is especially impactful for large models that can take several minutes to load.
- **Accelerated deployment:** You can deploy cached models instantly without waiting for external downloads or transfers.
- **Smaller container images:** By decoupling models from your container image, you can create smaller, more focused images that contain only your serving logic.

## How it works

When you select a cached model for your endpoint, Runpod automatically tries to start your workers on hosts that already contain the selected model.

If no cached host machines are available, the system delays starting your workers until the model is downloaded onto the machine where your workers will run, ensuring you still won't be charged for the download time.

## Enabling cached models

Follow these steps to select and add a cached model to your Serverless endpoint:

1. Navigate to the [Serverless section](https://www.console.runpod.io/serverless) of the Runpod console.
2. Click **New Endpoint**.
3. In the **Endpoint Configuration** step, scroll down to **Model (optional)** and add the link for the model you want to use (e.g. `https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct`).
4. Enter a Hugging Face token if you're using a gated model.
5. Complete your endpoint configuration and click **Deploy Endpoint**.

You can add a cached model to an existing endpoint by selecting **Manage â†’ Edit Endpoint** in the endpoint details page and updating the **Model (optional)** field.