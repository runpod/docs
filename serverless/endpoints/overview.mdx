---
title: "Overview"
sidebarTitle: "Overview"
description: "Deploy and manage Serverless endpoints using the Runpod console or REST API."
---

Endpoints are the foundation of Runpod Serverless, serving as the gateway for deploying and managing your Serverless workers. They provide a consistent API interface that allows your applications to interact with powerful compute resources on demand.

Whether you're processing large datasets, running AI inference, or performing compute-intensive tasks, endpoints give you the flexibility to deploy and scale your workloads.

## What are Serverless endpoints?

Runpod Serverless endpoints are RESTful APIs that accept HTTP requests, execute your code, and return the result via HTTP response. Each endpoint provides a unique URL and abstracts away the complexity of managing individual GPUs/CPUs.

Behind the scenes, Runpod handles the entire lifecycle of Serverless workers, including job queuing, execution, and result delivery, so you can focus on your code rather than the underlying infrastructure.

## Key features

### Execution modes

Serverless offers **asynchronous processing** via the `/run` endpoint operation, which lets you submit jobs that run in the background and check results later, making this ideal for long-running tasks.

It also provides **synchronous operations** through the `/runsync` endpoint operation, allowing you to receive immediate results in the same request, which is perfect for interactive applications.

To learn more, see [Operation overview](/serverless/endpoints/send-requests#operation-overview).

### Deployment and scaling

Runpod endpoints are **auto-scaling**, automatically scaling from zero to hundreds of workers based on demand. You can **customize your endpoint configuration** to adjust the minimum and maximum worker count, GPU allocation, and memory settings. The system also offers **GPU prioritization**, allowing you to specify preferred GPU types in order of priority.

To learn more, see [Endpoint settings](/serverless/endpoints/endpoint-configurations).

### Integration options

Runpod endpoints support [webhook notifications](/serverless/endpoints/send-requests#webhook-notifications), allowing you to configure endpoints to call your webhook when jobs complete.

It also includes [S3-compatible storage integration](/serverless/endpoints/send-requests#s3-compatible-storage-integration) for working with object storage for larger inputs and outputs.


## Create an endpoint

To deploy, make sure you have a worker deployed to your endpoint. See [Deploy workers](/serverless/workers/overview) for details.

<Tabs>
<Tab title="Web">
To create a new Serverless endpoint through the Runpod web interface:

1. Navigate to the [Serverless section](https://www.console.runpod.io/serverless) of the Runpod console.
2. Click **New Endpoint**.
3. On the **Deploy a New Serverless Endpoint** screen, choose your deployment source:
   * **Import Git Repository** (if GitHub is connected). See [Deploy from GitHub](/serverless/workers/github-integration) for details.
   * **Import from Docker Registry**. See [Deploy from Docker Hub](/serverless/workers/deploy) for details.
   * Or select a preconfigured endpoint under **Ready-to-Deploy Repos**.
4. Follow the UI steps to configure your selected source (Docker image, GitHub repo), then click **Next**.
5. Configure your endpoint settings:
   * **Endpoint Name**: The display name for your endpoint in the console.
   * **Endpoint Type**: Select **Queue** for traditional queue-based processing or **Load balancer** for direct HTTP access. See [Load balancing endpoints](/serverless/load-balancing/overview) for details.
   * **GPU Configuration**: Select the appropriate GPU types and configure worker settings.
   * **Model**: (Optional) Enter a model URL from Hugging Face to optimize worker startup times. See [Cached models](/serverless/endpoints/model-caching) for details.
   * **Container Configuration**: Edit the container start command, specify the [container disk size](/serverless/storage/overview), and expose HTTP/TCP ports.
   * **Environment Variables**: Add [environment variables](/serverless/development/environment-variables) for your worker containers.
6. Click **Deploy Endpoint** to deploy.

</Tab>

<Tab title="REST API">
To create a Serverless endpoint using the REST API, send a POST request to the `/endpoints` endpoint:

```bash
curl --request POST \
  --url https://rest.runpod.io/v1/endpoints \
  --header 'Authorization: Bearer RUNPOD_API_KEY' \
  --header 'Content-Type: application/json' \
  --data '{
  "allowedCudaVersions": [
    "12.8"
  ],
  "computeType": "GPU",
  "cpuFlavorIds": [
    "cpu3c"
  ],
  "dataCenterIds": [
    "EU-RO-1",
    "CA-MTL-1"
  ],
  "executionTimeoutMs": 600000,
  "flashboot": true,
  "gpuCount": 1,
  "gpuTypeIds": [
    "NVIDIA GeForce RTX 4090"
  ],
  "idleTimeout": 5,
  "name": "my-endpoint",
  "scalerType": "QUEUE_DELAY",
  "scalerValue": 4,
  "templateId": "30zmvf89kd",
  "vcpuCount": 2,
  "workersMax": 3,
  "workersMin": 0
}'
```

For complete API documentation and parameter details, see the [Serverless endpoint API reference](/api-reference/endpoints/POST/endpoints).

</Tab>

</Tabs>

<Tip>
You can optimize cost and availability by specifying GPU preferences in order of priority. Runpod attempts to allocate your first choice GPU. If unavailable, it automatically uses the next GPU in your priority list, ensuring your workloads run on the best available resources.

You can enable or disable particular GPU types using the **Advanced > Enabled GPU Types** section.
</Tip>

After deployment, your endpoint takes time to initialize before it is ready to process requests. You can monitor the deployment status on the endpoint details page, which shows worker status and initialization progress. Once active, your endpoint displays a unique API URL (`https://api.runpod.ai/v2/{endpoint_id}/`) that you can use to send requests.

## Edit an endpoint

<Frame alt="Endpoint configurations">
  <img src="/images/endpoint-settings.png" />
</Frame>

You can modify your endpoint's configuration at any time:

1. Navigate to the [Serverless section](https://www.console.runpod.io/serverless) in the Runpod console.
2. Click the three dots in the bottom right corner of the endpoint you want to modify.
3. Click **Edit Endpoint**.
4. Update any [configuration parameters](/serverless/endpoints/endpoint-configurations) as needed.
5. Click **Update Endpoint** to save your changes.

Changes to some settings (like GPU types or worker counts) may require restarting active workers to take effect.

## Delete an endpoint

To delete an endpoint:

1. Navigate to the [Serverless section](https://www.console.runpod.io/serverless) in the Runpod console.
2. Click the three dots in the bottom right corner of the endpoint you want to delete.
3. Click **Delete Endpoint**.
4. Confirm the deletion.

<Warning>
Deleting an endpoint permanently removes all configuration, logs, and job history. This action cannot be undone.
</Warning>

## Next steps

* [Send requests to your endpoint](/serverless/endpoints/send-requests)
* [Configure endpoint settings](/serverless/endpoints/endpoint-configurations)
* [Monitor job states and metrics](/serverless/endpoints/job-states)
* [Optimize endpoint performance](/serverless/development/optimization)
