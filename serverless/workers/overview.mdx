---
title: "Overview"
description: "Package and deploy your code to Serverless workers."
---

Workers are the containerized environments that run your code on Runpod Serverless. After creating and testing your [handler function](/serverless/workers/handler-functions), you need to package it into a Docker image and deploy it to an endpoint.

Runpod automatically manages the worker lifecycleâ€”starting workers when requests arrive, keeping them active to handle additional requests, and shutting them down when idle to minimize costs.

## Deployment options

You can deploy workers to Runpod Serverless in two ways:

### Deploy from Docker Hub 

Build your Docker image locally, push it to Docker Hub (or another container registry), and deploy it to Runpod. This gives you full control over the build process and allows you to test images locally before deployment.

See [Deploy from Docker Hub](/serverless/workers/deploy) for details.

### Deploy from GitHub

Connect your GitHub repository to Runpod and deploy directly from your code. Runpod automatically builds your Docker image from your repository and deploys it to an endpoint. This streamlines the deployment process and enables continuous deployment workflows.

See [Deploy from GitHub](/serverless/workers/github-integration) for details.

## Worker configuration

When deploying a worker, you can configure:

* **GPU/CPU types**: Select specific GPU models for your workload.
* **Worker count**: Set minimum and maximum worker counts for auto-scaling.
* **Container disk**: Allocate temporary storage for your worker. See [Storage options](/serverless/storage/overview).
* **Environment variables**: Pass configuration values to your worker. See [Environment variables](/serverless/development/environment-variables).
* **Model caching**: Pre-load models to reduce cold start times. See [Cached models](/serverless/endpoints/model-caching).

These settings are configured when you [create an endpoint](/serverless/endpoints/overview).

## Active vs. flex workers

* **Active (min) workers**: "Always on" workers that eliminate cold start delays. The system charges you immediately but offers up to 30% discount. (Default: 0).
* **Flex workers**: "Sometimes on" workers that scale during traffic surges. They transition to idle after completing jobs. (Default: Max - Active = 3).
* **Extra workers**: Additional workers that the system adds during traffic spikes when Docker images are cached on host servers. (Default: 2).

## Worker states

Workers move through different states as they handle requests and respond to changes in traffic patterns. Understanding these states helps you monitor and troubleshoot your endpoints effectively.

* **Initializing**: The worker starts up while the system downloads and prepares the Docker image. The container starts and loads your code.
* **Idle**: The worker is ready but not processing requests. No charges apply while idle.
* **Running**: The worker actively processes requests. Billing occurs per second.
* **Throttled**: The worker is ready but temporarily unable to run due to host machine resource constraints.
* **Outdated**: The system marks the worker for replacement after endpoint updates. It continues processing current jobs during rolling updates (10% of max workers at a time).
* **Unhealthy**: The worker has crashed due to Docker image issues, incorrect start commands, or machine problems. The system automatically retries with exponential backoff for up to 7 days.

You can view the state of your workers using the **Workers** tab of a Serverless endpoint. This tab provides real-time information about each worker's current state, resource utilization, and job processing history, allowing you to monitor performance and troubleshoot issues effectively.

## Debugging workers

To debug issues in production, you can access [worker logs](/serverless/development/logs) and [SSH directly into running workers](/serverless/development/ssh-into-workers) to inspect file systems and environment variables in real-time.

## Max worker limit

By default, each Runpod account can allocate a maximum of 5 workers (flex + active combined) across all endpoints. If your account balance exceeds a certain threshold, you can increase this limit:

- \$100 balance: 10 max workers
- \$200 balance: 20 max workers
- \$300 balance: 30 max workers
- \$500 balance: 40 max workers
- \$700 balance: 50 max workers
- \$900 balance: 60 max workers

If your workload requires additional capacity beyond 60 workers, [contact our support team](https://www.runpod.io/contact).

## Best practices

Follow these best practices when deploying workers:

* **Optimize image size**: Smaller images download faster and reduce cold start times. See [Create a Dockerfile](/serverless/workers/create-dockerfile) for optimization techniques.
* **Use model caching**: Pre-load models to avoid downloading them on every cold start. See [Cached models](/serverless/endpoints/model-caching).
* **Test locally first**: Always test your handler locally before deploying. See [Local testing](/serverless/development/local-testing).
* **Handle errors gracefully**: Implement proper error handling to prevent worker crashes. See [Error handling](/serverless/development/error-handling).
* **Monitor logs**: Use logs to debug and optimize your workers. See [Logs](/serverless/development/logs).


## Next steps

* [Write a handler function](/serverless/workers/handler-functions)
* [Create a Dockerfile for your worker](/serverless/workers/create-dockerfile)
* [Deploy from Docker Hub](/serverless/workers/deploy)
* [Deploy from GitHub](/serverless/workers/github-integration W