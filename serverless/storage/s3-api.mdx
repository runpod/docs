---
title: "Manage network volumes with S3-compatible API"
sidebarTitle: "S3-compatible API"
description: "Use RunPod's S3-compatible API to access and manage your network volumes."
---

RunPod provides an S3-protocol compatible API that allows direct access to your [network volumes](/serverless/storage/network-volumes). This feature enables you to manage files on your network volumes without needing to launch a Pod, reducing cost and operational friction.

Pricing for S3-compatible API usage is the same as for network volumes.

## Setup and authentication

<Steps>
  <Step title="Create a shared key">
    To use the S3-compatible API, you must generate a new key called a "shared key", separate from your RunPod API key.

    1. In the RunPod console, navigate to the [Settings page](https://www.runpod.io/console/user/settings).
    2. Expand the **Shared Keys** section and select **Create a shared key**.
    3. Give your key a name and select **Create**.
    4. Make a note of your access key and secret access key to use in the next step.

    <Warning>

    RunPod does not store your secret access key, so you may wish to save it elsewhere (e.g., in your password manager, or in a GitHub secret). Treat your secret access key like a password and don't share it with anyone.

    </Warning>
  </Step>

  <Step title="Configure AWS CLI">
    To use the S3-compatible API with your RunPod network volumes, you must configure your AWS CLI with the RunPod shared key you created.

    1.  If you haven't already, [install the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) on your local machine.
    2.  Run the command `aws configure` in your terminal.
    3.  Provide the following when prompted:
        *   **AWS Access Key ID**: Enter your RunPod access key.
        *   **AWS Secret Access Key**: Enter your RunPod secret access key.
        *   **Default Region name**: You can leave this blank.
        *   **Default output format**: You can leave this blank or set it to `json`.

    This will configure the AWS CLI to use your RunPod shared key by storing these details in your AWS credentials file (typically at `~/.aws/credentials`).
    
  </Step>
</Steps>

## Using the S3-compatible API

You can use the S3-compatible API to interact with your RunPod network volumes using standard S3 tools:

*   [AWS CLI commands](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/s3/index.html).
*   [The Boto3 Python library](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html).

Core AWS CLI operations such as `ls`, `cp`, `mv`, `rm`, and `sync` function as expected.

## AWS CLI examples

When using AWS CLI commands, you must pass in the URL for the RunPod datacenter where your network volume is located using the `--endpoint-url` parameter. The URL for your datacenter uses the format `https://[DATACENTER]-s3api.runpod.dev/`, for example `https://ca-qc-1-s3api.runpod.dev/`.

Your datacenter ID can be found in the [Storage section](https://www.runpod.io/console/user/storage) of the RunPod console.

### List objects

Use `ls` to list objects in a network volume directory:

```bash
aws s3 ls --endpoint-url https://[DATACENTER]-s3api.runpod.dev/ \ 
    s3://[NETWORK_VOLUME_ID]/[REMOTE_DIR]
```

### Transfer files

Use `cp` to copy a file to a network volume:

```bash
aws s3 cp --endpoint-url https://[DATACENTER]-s3api.runpod.dev/ \
    local-file.txt \
    s3://[NETWORK_VOLUME_ID]
```

Use `cp` to copy a file from a network volume to a local directory:

```bash
aws s3 cp --endpoint-url https://[DATACENTER]-s3api.runpod.dev/ \
    s3://[NETWORK_VOLUME_ID]/remote-file.txt ./[LOCAL_DIR]
```

Use `rm` to remove a file from a network volume:

```bash
aws s3 rm --endpoint-url https://[DATACENTER]-s3api.runpod.dev/ \
    s3://[NETWORK_VOLUME_ID]/remote-file.txt
```

### Sync directories

This command syncs a local directory (source) to a network volume directory (destination):

```bash
aws s3 sync --endpoint-url https://[DATACENTER]-s3api.runpod.dev/ \
    ./[LOCAL_DIR] \
    s3://[NETWORK_VOLUME_ID]/[REMOTE_DIR]
```

## Boto3 Python example

You can also use the Boto3 library to interact with the S3-compatible API, using it to transfer files to and from a RunPod network volume.

The following script demonstrates how to upload a file to a RunPod network volume using the Boto3 library:

```python
#!/usr/bin/env python3

import os
import argparse

import boto3

def create_s3_client(region, endpoint_url):
    aws_access_key_id = os.environ.get("RUNPOD_USER_EMAIL")
    aws_secret_access_key = os.environ.get("RUNPOD_S3_API_KEY")

    if not aws_access_key_id or not aws_secret_access_key:
        raise EnvironmentError(
            "Please set RUNPOD_USER_EMAIL and RUNPOD_S3_API_KEY environment variables"
        )

    return boto3.client(
        "s3",
        aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key,
        region_name=region,
        endpoint_url=endpoint_url,
    )

def put_object(s3_client, bucket_name, object_name, file_path):
    try:
        s3_client.upload_file(file_path, bucket_name, object_name)
        print(f"Uploaded {file_path} to s3://{bucket_name}/{object_name}")
    except Exception as e:
        print(f"Error uploading file: {e}")
        raise

def main():
    parser = argparse.ArgumentParser(
        description="Upload a file to S3-compatible storage"
    )
    parser.add_argument("-b", "--bucket", required=True, help="Bucket name")
    parser.add_argument("-e", "--endpoint", required=True, help="S3 endpoint URL")
    parser.add_argument(
        "-f", "--file", required=True, help="Path to the file to upload"
    )
    parser.add_argument("-o", "--object", required=True, help="S3 object key")
    parser.add_argument("-r", "--region", required=True, help="AWS region name")

    args = parser.parse_args()

    client = create_s3_client(args.region, args.endpoint)
    put_object(client, args.bucket, args.object, args.file)

if __name__ == "__main__":
    main()

```

Example usage:

```bash
./s3_example_put.py --endpoint https://ca-mtl-1-s3api.runpod.io/ \
    --region 'ca-mtl-1' \
    --bucket 'network_volume_id' \
    --object 'path/to/model_file.bin' \
    --file 'model_file.bin'
```

## Supported S3 actions

The S3-compatible API supports the following operations. For detailed information on each, refer to the [AWS S3 API documentation](https://docs.aws.amazon.com/AmazonS3/latest/API/API_Operations_Amazon_Simple_Storage_Service.html).

| Operation                 | Description                                  |
| ------------------------- | -------------------------------------------- |
| `CopyObject`             | Copy objects between locations.              |
| `DeleteObject`           | Remove objects.                              |
| `GetObject`              | Download objects.                            |
| `HeadBucket`             | Verify bucket exists and you have permissions. |
| `HeadObject`             | Retrieve object metadata.                    |
| `ListBuckets`            | List available buckets.                      |
| `ListObjects`            | List objects in a bucket.                    |
| `PutObject`              | Upload objects.                              |
| `CreateMultipartUpload` | Start a multipart upload for large files.    |
| `UploadPart`             | Upload a part of a multipart upload.         |
| `CompleteMultipartUpload`| Finish a multipart upload.                   |
| `AbortMultipartUpload`  | Cancel a multipart upload.                   |
| `ListMultipartUploads`  | View in-progress multipart uploads.          |

Large file handling is supported through multipart uploads, allowing you to transfer files larger than 5GB.

## Usage guidelines and limitations

When working with the S3-compatible API, remember the following:

*   **Object names**: Unlike traditional S3 key-value stores, RunPod object names in correspond to actual file paths on your network volume. Object names containing special characters (e.g., `#`) may need to be URL encoded to ensure proper processing.
*   **Multipart uploads**:
    *   Parts from multipart uploads are stored on disk until either `CompleteMultipartUpload` or `AbortMultipartUpload` is called.
    *   The API enforces the 5GB maximum single file part upload size, but not the 5TB maximum file size.
    *   The S3 specification's 5MB minimum part size for multipart uploads is not enforced.
*   **Storage capacity**: Network volumes have a fixed storage capacity, unlike the virtually unlimited storage of standard S3 buckets.
    *   `CopyObject` and `UploadPart` actions do not check for available free space beforehand and may fail if the volume runs out of space. This behavior is similar to applying a size quota in S3.
*   **Time synchronization**: Requests that are out of time sync by 1 hour will be rejected. This is more lenient than the 15-minute window specified by the AWS SigV4 authentication specification.

## Reference documentation

For comprehensive documentation on AWS S3 commands and libraries, refer to:

- [AWS CLI S3 reference](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/s3/index.html).
- [AWS S3 API reference](https://docs.aws.amazon.com/AmazonS3/latest/API/API_Operations_Amazon_Simple_Storage_Service.html).
- [Boto3 S3 reference](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html).
