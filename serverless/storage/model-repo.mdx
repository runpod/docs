---
title: "Private model repository"
sidebarTitle: "Model repository"
description: "Upload models to Runpod to speed up worker starts and reduce costs for your Serverless endpoints."
tag: "BETA"
---

<Note>

The private model repository feature is currently in beta. Please [join our Discord](https://discord.gg/runpod) if you'd like to provide feedback.

</Note>

This guide an overview of how the private model repository works, and instructions for managing your models with the [Runpod CLI](/runpodctl/overview).

The Runpod private model repository allows you to upload your models directly to the Runpod ecosystem. By pre-caching models on our infrastructure, you can significantly reduce worker start times, lower costs, and improve the reliability of your Serverless endpoints.

## Overview

Using the private model repository provides several key advantages:

- **Faster cold start times:** Models stored in the repository are pre-cached on Runpod's infrastructure, eliminating the need for workers to download them from external sources like Hugging Face. Our smart scheduler also prioritizes placing your jobs on machines that already have the required model, leading to near-instantaneous starts.
- **Reduced costs:** You aren't billed for worker time while your model is being downloaded. This is especially impactful for large models that can take several minutes to load.
- **Improved reliability:** Storing models on Runpod reduces your dependency on external services, which can experience downtime or rate-limiting.
- **Smaller container images:** By decoupling models from your container image, you can create smaller, more focused images that contain only your serving logic.

## Manage private models

Models can be uploaded and managed using the Runpod CLI (`runpodctl`). Make sure you've [installed runpodctl](/runpodctl/install-runpodctl) and [configured your API key](/runpodctl/overview#configure-your-api-key).

### Upload a model

You can upload any model from the [Hugging Face Model Hub](https://huggingface.co/models) to the Runpod repository using the model identifier.

To upload a model from Hugging Face, run the following command:

```bash
runpodctl create model \
    --provider huggingface \
    --name YOUR_MODEL_NAME
```

Replace `YOUR_MODEL_NAME` with the model identifier from Hugging Face.

For example, to upload the `stable-diffusion-xl-refiner-1.0` model, run:

```bash
runpodctl create model \
    --provider huggingface \
    --name stabilityai/stable-diffusion-xl-refiner-1.0

```

### List your uploaded models

To see a list of all models you've uploaded to the repository, run the following command:

```bash
runpodctl get models
```

This will display all the models in your repository, allowing you to confirm successful uploads and check for duplicates.

You should see output similar to the following:

```bash
ID         NAME              SOURCE          STATUS     SIZE(GB)  VERSION(SHORT)  
mdl_123    custom-llama-v1   HUGGING_FACE        READY      24.7        9f1c2ab           
mdl_456    llama31-8b        HUGGING_FACE    DOWNLOADING    -           -          
```

### Get model details

To get detailed information about a specific model, run:

```bash
runpodctl get model YOUR_MODEL_ID
```

Replace `YOUR_MODEL_ID` with the ID of your uploaded model.

For example, running `runpodctl get model 4oqrsweux0fkcp` on the example output above would return:

```shell
provider:   huggingface
name:   stabilityai/stable-diffusion-xl-refiner-1.0
createdDate:    2023-08-03T22:31:36.289Z
storagePath:    /stabilityai-stable-diffusion-xl-refiner-1.0/
id:             4oqrsweux0fkcp
bucketId:       pllmb-staging-cloud
regionSpecs:
- regionName:   Staging
  bucketName:    pllmb-staging-cloud
  multiplier:    8
  maxQuantity:   30
  maxIncrement:  5
  amount:        22
```
### Remove a model

When you no longer need a model uploaded to the global repository, you can remove it using `runpodctl`. This cleans up your repository list and frees up storage space.

To remove a model, run the following command:

```bash
runpodctl remove model \
  --provider huggingface \
    --name lodestones/Chroma
```

<Warning>
Before removing a model, ensure that none of your active endpoints are using it.
</Warning>

## Use models in Serverless endpoints

Once you've uploaded a model to the repository, you can use it in your Serverless endpoints to improve initialization performance.

### Select private repository models when creating endpoints

When creating a new Serverless endpoint or updating an existing one, you can select models from your organization repository.

To select a model from the repository, perform the following steps:

1. In the [Runpod console](https://www.console.runpod.io/serverless), go to **Serverless** > **Endpoints**.
2. Click **New Endpoint**, or edit an existing endpoint.
3. In the **Endpoint Configuration** screen, scroll down to **Model (optional)** and click the dropdown. Your uploaded models will be listed under **Organization Repository**.
4. Select your model from the list.
5. Complete your endpoint configuration and click **Create Endpoint**.

## How it works

### Smart placement

When you use a cached model at endpoint creation time, Runpod automatically checks throughout the infrastructure for hosts that already contain your selected model.

If no pre-cached host workers are available, the system still gracefully provides fallbacks and will download the model to unoccupied workers, prioritizing local access.

### Graceful fallback

For scenarios where pre-cached models aren't immediately available, Runpod implements intelligent fallback handling:

For uncached models:
- Workers are started immediately during endpoint creation when the specs are selected.
- Model download happens concurrently in the background, then the download process passes the right weights and config to the container code after extraction.

For cached models:
- Smart placement: Will check if it exists on machine.
- Placement with pre-waiting is essentially identical to a fully downloaded model.

The model caching process is largely transparentâ€”you simply specify the desired cached model for your endpoint with deployment, and Runpod handles recognizing and optimizing both initial downloads and subsequent cached access.

## Pricing

The private model repository feature is available at no additional cost during the beta launch period.